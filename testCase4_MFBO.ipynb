{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this notebook, we illustrate how to implement a **Multi-Fidelity Bayesian Optimization** (MFBO) method.\n",
        "\n",
        "We set up first a PyTorch environment for use with the Botorch library for *Bayesian optimization* research."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        },
        "id": "fJ1_RXzgs_FN",
        "outputId": "68d622d7-0e37-44af-fdd9-f4b7b3cf9d98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on PyTorch 1.12.1, Botorch 0.8.1, using cpu device\n"
          ]
        }
      ],
      "source": [
        "import os, torch, botorch, gpytorch\n",
        "torch.set_default_dtype(torch.double)\n",
        "\n",
        "SMOKE_TEST = os.environ.get(\"SMOKE_TEST\")\n",
        "\n",
        "# Get cpu or gpu device for training\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
        "print(f'Running on PyTorch {torch.__version__}, Botorch {botorch.__version__}, using {device} device')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem setup\n",
        "Load and preprocess data from a CSV file for use in the model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "data = 'input_data/Case3_finer.csv'\n",
        "# Data loading\n",
        "def load_data(data=data):\n",
        "    df = pd.read_csv(data)\n",
        "    X_data  = np.vstack([df[\"alpha0\"].to_numpy() ,\n",
        "                df[\"alpha1\"].to_numpy() ,\n",
        "                df[\"alpha2\"].to_numpy() ,\n",
        "                df[\"alpha3\"].to_numpy() ])\n",
        "    pts = X_data.T.copy()\n",
        "    pts = StandardScaler().fit_transform(pts)\n",
        "    '''COMPUTE DIFFERENT CASES AND PLOT THEM'''\n",
        "    Cx_data = df['SumCx'].to_numpy() \n",
        "\n",
        "\n",
        "    #Cx_data = np.vstack([df['SumCx'].to_numpy() ,\n",
        "    #                     df['SumCy'].to_numpy() ])\n",
        "\n",
        "    \n",
        "    if Cx_data.ndim < 2:\n",
        "       obs = Cx_data.reshape(-1,1).copy() \n",
        "    else:\n",
        "       obs = Cx_data.T.copy()\n",
        "\n",
        "    obs = StandardScaler().fit_transform(obs)\n",
        "    return pts, obs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Perform a PCA transformation on the given dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "   \n",
        "# Compute PCA \n",
        "def PCA_transformation(pts_original):\n",
        "    pca = PCA(n_components=4, svd_solver='full').fit(pts_original)\n",
        "    pts_transformed = pca.transform(pts_original)\n",
        "    return pts_transformed"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Resample data using a Latin hypercube sample and a nearest neighbor interpolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyDOE import lhs\n",
        "from scipy.interpolate import NearestNDInterpolator\n",
        "\n",
        "# Sample RSMs\n",
        "def data_resample(pts, obs):\n",
        "    dim = pts.shape[1]\n",
        "    N = 10000\n",
        "    lb = np.min( pts,axis=0)\n",
        "    ub = np.max( pts,axis=0)\n",
        "    bounds = {'lb': lb, 'ub': ub}\n",
        "    # Generate latin-hypercube\n",
        "    new_pts = lb + (ub - lb) * lhs(dim, N) \n",
        "    # pts are not in convex hull of pts (LD interpolator does not extrapolate) \n",
        "    r =  NearestNDInterpolator( pts, obs)\n",
        "    # pts has to be inside region of interpolation .\n",
        "    valuesTrasf = r(new_pts) \n",
        "    #valuesTrasf.reshape(-1,1).T\n",
        "    return new_pts,valuesTrasf"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a PyTorch Dataset for the test case, which will be used for training and testing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "pts, obs = load_data()\n",
        "pts = PCA_transformation(pts)\n",
        "new_pts,new_obs = data_resample(pts, obs)\n",
        "\n",
        "class TestcaseDataset(Dataset):\n",
        "    def __init__(self,new_pts,new_obs,dim):\n",
        "        data = np.hstack([new_pts,new_obs]) \n",
        "        hifi, lofi = train_test_split(data, test_size=1e-3, shuffle=True)\n",
        "        hifi, test = train_test_split(hifi, test_size=1e-1)\n",
        "        size=dim-data.shape[1]\n",
        "        # Cast them\n",
        "        self.X_hifi = torch.Tensor(hifi[:,:size]) \n",
        "        self.X_lofi = torch.Tensor(lofi[:,:size]) \n",
        "        self.X_test = torch.Tensor(test[:,:size]) \n",
        "        self.Y_hifi = torch.Tensor(hifi[:,size:])#.unsqueeze(-1)\n",
        "        self.Y_lofi = torch.Tensor(lofi[:,size:])#.unsqueeze(-1)\n",
        "        self.Y_test = torch.Tensor(test[:,size:])#.unsqueeze(-1)\n",
        "\n",
        "        self.hifi_dataset = TensorDataset(self.X_hifi, self.Y_hifi)\n",
        "        self.test_dataset = TensorDataset(self.X_test, self.Y_test)\n",
        "    def __call__(self):\n",
        "        return (self.hifi_dataset, \n",
        "                self.test_dataset, \n",
        "                self.X_hifi, \n",
        "                self.X_lofi, \n",
        "                self.X_test, \n",
        "                self.Y_hifi, \n",
        "                self.Y_lofi, \n",
        "                self.Y_test, \n",
        "                )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initialize the data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from botorch.models.transforms.outcome import Standardize\n",
        "from botorch.utils.transforms import unnormalize, normalize\n",
        "dataset = TestcaseDataset(new_pts,new_obs,pts.shape[1])\n",
        "\n",
        "X_lofi = dataset.X_lofi#normalize(dataset.X_lofi,bounds=bounds)\n",
        "X_hifi = dataset.X_hifi#normalize(dataset.X_hifi,bounds=bounds)\n",
        "X_test = dataset.X_test#normalize(dataset.X_test,bounds=bounds)\n",
        "Y_lofi = dataset.Y_lofi\n",
        "Y_hifi = dataset.Y_hifi\n",
        "Y_test = dataset.Y_test\n",
        "\n",
        "hifi_dataset = dataset.hifi_dataset\n",
        "test_dataset = dataset.test_dataset\n",
        "\n",
        "#bounds = torch.stack([-2.5 * torch.ones(pts.shape[-1]), 2.5 * torch.ones(pts.shape[-1])])\n",
        "bounds = torch.Tensor([[pts[:,0].min(), pts[:,1].min(), pts[:,2].min(), pts[:,3].min()], [pts[:,0].max(), pts[:,1].max(), pts[:,2].max(), pts[:,3].max()]])\n",
        "'''print(\"Shape of low fidelity  X and y: \",X_lofi.shape, Y_lofi.shape)\n",
        "print(\"Shape of high fidelity X and y: \",X_hifi.shape, Y_hifi.shape)\n",
        "print(\"Shape of test set      X and y: \",X_test.shape, Y_test.shape)\n",
        "print(\"Buonds shape: \", bounds.shape)\n",
        "print('points shape',pts.shape)'''\n",
        "\n",
        "# Create data loaders\n",
        "hifi_dataloader = DataLoader(hifi_dataset, batch_size = 32, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size = 32, shuffle=True)\n",
        "best_obs_value = Y_hifi.numpy().max()\n",
        "\n",
        "NOISE_LEVEL = 1e-1\n",
        "noise = NOISE_LEVEL*torch.ones(Y_lofi.shape[0])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display the data points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn')\n",
        "\n",
        "fig = plt.figure(figsize=(21, 9))\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "ax.scatter(X_hifi[:, 0],X_hifi[:, 1],X_hifi[:, 2],label = 'high fidelity')\n",
        "ax.scatter(X_test[:, 0],X_test[:, 1],X_test[:, 2],label = 'test')\n",
        "ax.scatter(X_lofi[:, 0],X_lofi[:, 1],X_lofi[:, 2],label = 'low fidelity')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## High Fidelity Model\n",
        "\n",
        "Initialize a `high fidelity neural network` model, which uses the *Mean Square Error* (MSE) loss and the *Adam Optimizer* for training. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch import nn # Neural Network Module\n",
        "# Define hyperparameters\n",
        "n_samples, n_features = X_hifi.shape\n",
        "input_size = n_features\n",
        "hidden_size = 128\n",
        "output_size = Y_hifi.shape[1]\n",
        "learning_rate = 1e-2\n",
        "# Define the neural network model (input, hidden, output size)\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self,input_size,hidden_size,output_size):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        # Define layers\n",
        "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.LeakyReLU() # activation function nn.LeakyReLU()\n",
        "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        out = self.linear1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.linear2(out)\n",
        "        # Uncomment the following line if your task requires a sigmoid activation on the output\n",
        "        # out = self.sigmoid(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### variant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define hyperparameters\n",
        "n_samples, n_features = X_hifi.shape\n",
        "input_size = n_features\n",
        "hidden_size = 128\n",
        "output_size = Y_hifi.shape[1]\n",
        "learning_rate = 1e-2\n",
        "\n",
        "# Define the neural network model\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        # Define layers\n",
        "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.LeakyReLU()  # Activation function\n",
        "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.linear2(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Instantiate the model\n",
        "model = NeuralNet(input_size, hidden_size, output_size)\n",
        "\n",
        "# Print the model architecture\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### variant 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers):\n",
        "        super(Encoder, self).__init__()\n",
        "        layers = []\n",
        "        for i in range(num_layers):\n",
        "            layers.append(nn.Conv1d(in_channels=input_dim if i == 0 else hidden_dim, \n",
        "                                    out_channels=hidden_dim, \n",
        "                                    kernel_size=kernel_size, \n",
        "                                    padding=kernel_size // 2))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
        "        self.encoder = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, hidden_dim, kernel_size, num_layers):\n",
        "        super(Decoder, self).__init__()\n",
        "        layers = []\n",
        "        for i in range(num_layers):\n",
        "            layers.append(nn.Conv1d(in_channels=hidden_dim, \n",
        "                                    out_channels=hidden_dim, \n",
        "                                    kernel_size=kernel_size, \n",
        "                                    padding=kernel_size // 2))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
        "        layers.append(nn.Conv1d(in_channels=hidden_dim, \n",
        "                                out_channels=output_dim, \n",
        "                                kernel_size=kernel_size, \n",
        "                                padding=kernel_size // 2))\n",
        "        self.decoder = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.decoder(x)\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, kernel_size=3, num_layers=1):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.encoder = Encoder(input_dim, hidden_dim, kernel_size, num_layers)\n",
        "        self.decoder = Decoder(output_dim, hidden_dim, kernel_size, num_layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "# Example usage\n",
        "input_dim = 1  # Number of input features\n",
        "output_dim = 1  # Number of output features\n",
        "hidden_dim = 64  # Number of features in the hidden layers\n",
        "kernel_size = 3  # Size of the convolutional kernel\n",
        "num_layers = 3  # Number of layers in the encoder and decoder\n",
        "\n",
        "model = NeuralNet(input_dim, hidden_dim, output_dim, kernel_size=3, num_layers=1)\n",
        "print(model)\n",
        "\n",
        "# Assuming x is your input tensor with shape (batch_size, input_dim, sequence_length)\n",
        "x = torch.randn(32, input_dim, 100)  # Example input\n",
        "output = model(x)\n",
        "print(output.shape)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We train the model for a specified number of epochs on the training data. The data is fed to the model in batches. The model makes predictions on the training data and backpropagates the prediction error to update the model parameters. The loss is printed after every 10 epochs.\n",
        "\n",
        "It is possible to save the state of the neural network to a file after training. \n",
        "\n",
        "It is possible to import the state of a previously trained neural network model. The `NeuralNet` class, defined earlier, takes the same *input size*, *hidden size*, and *output size* as the saved model. The `load_state_dict` method is used to load the saved state of the neural network from a file. In this case, the file is located in the `model_weights` directory. Once the model is loaded, it is set to evaluation mode using the `eval()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Import the state of the neural network model saved\n"
          ]
        }
      ],
      "source": [
        "model_file = \"trained_model_weights/testcase_nn_PCA_3000_singlobj.pt\"\n",
        "path = os.path.join(os.getcwd(), model_file)\n",
        "\n",
        "if os.path.isfile(path):\n",
        "    # Load model weights from the work directory\n",
        "    print('State dict of the neural network model imported')\n",
        "    nn_model = NeuralNet(input_size,hidden_size,output_size).to(device)\n",
        "    nn_model.load_state_dict(torch.load(path))\n",
        "    nn_model.eval()\n",
        "else:\n",
        "    print('Training the neural network model')\n",
        "    # Define a neural network model with input size, hidden size, and output size, then move it to the device (CPU or GPU)\n",
        "    nn_model = NeuralNet(input_size,hidden_size,output_size).to(device)\n",
        "    print(nn_model)\n",
        "\n",
        "    ''' Construct loss and optmizer '''\n",
        "    # Define a loss function for the neural network to minimize. Mean Square Error Loss is used in this case.\n",
        "    criterion = nn.MSELoss() # loss_fn = nn.CrossEntropyLoss()\n",
        "    # Define an optimizer to update the parameters of the neural network during training. Adam optimizer is used in this case.\n",
        "    optimizer = torch.optim.Adam(nn_model.parameters(), lr=learning_rate) \n",
        "\n",
        "    ''' In a single training epoch, the model makes predictions on the training dataset (fed to it in batches)\n",
        "    and backpropagates the prediction error to update the model’s parameters '''\n",
        "\n",
        "    # Set the number of epochs for training the neural network. In each epoch, the neural network is trained on all training samples in batches.\n",
        "    NUM_EPOCHS = 3000 if not SMOKE_TEST else 5 # epoch -> forward and backward of ALL training samples\n",
        "    # 3000 iter, single obj --> 17.58min\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        size = len(hifi_dataloader.dataset)\n",
        "        nn_model.train()\n",
        "        for batch, (X, y) in enumerate(hifi_dataloader):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            ''' Compute prediction error '''\n",
        "            # Forward pass the input data through the neural network to compute the predicted output.\n",
        "            pred = nn_model(X)         \n",
        "            # Compute the loss between the predicted output and the true output.\n",
        "            loss = criterion(pred, y) \n",
        "\n",
        "            ''' Backpropagation '''\n",
        "            # Clear the gradients from the previous iteration.\n",
        "            optimizer.zero_grad()   \n",
        "            # Compute the gradients for all the parameters in the neural network using backpropagation.\n",
        "            loss.backward()          \n",
        "            # Update the parameters of the neural network using the computed gradients.\n",
        "            optimizer.step()         \n",
        "\n",
        "        # Print the loss after every 10 epochs.   \n",
        "        if (epoch+1) % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1} / {NUM_EPOCHS}, step [{batch+1}/{len(hifi_dataloader)}] loss: {loss:.4f}\")\n",
        "\n",
        "    ''' SAVE THE MODEL'''\n",
        "    # Save the state of the neural network to a file.\n",
        "    cwd = os.getcwd()\n",
        "    model_file = \"trained_model_weights/testcase_nn.pt\" # STATE THE MODEL SPECIFICATIONS IN THE FILE NAME \n",
        "    torch.save(nn_model.state_dict(), os.path.join(cwd, model_file))\n",
        "    print('State dict of the neural network model saved')\n",
        "\n",
        "\n",
        "    ''' Test '''    \n",
        "    # Set the neural network to evaluation mode to disable the gradient calculation for test dataset.\n",
        "    nn_model.eval()\n",
        "    size = len(test_dataloader.dataset)\n",
        "    num_batches = len(test_dataloader)\n",
        "    test_loss, correct, num_samples = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in test_dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            # Forward pass the input data through the neural network to compute the predicted output.\n",
        "            pred = nn_model(X)\n",
        "            # Compute the loss between the predicted output and the true output.\n",
        "            test_loss += criterion(pred, y).item()\n",
        "            #num_samples += y.shape[0]\n",
        "            #correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    # Compute the average test loss.\n",
        "    test_loss /= num_batches\n",
        "    #correct /= size\n",
        "    #acc = 100.0 * correct / num_samples\n",
        "    # Print the test loss.\n",
        "    print(f\"Test Error -> Test loss: {test_loss:>8f} \\n\") # Acc: {acc:>8f}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualizes the neural network prediction compared to the test data. First we load the saved state of the neural network model using `load_state_dict` and set the model to evaluation mode using `eval()`. Then, using the `torch.no_grad()` context manager, the model predicts the output *Y_pred* for the test data *X_test*. Finally, we create a plot to compare the predicted output to the true output for each target variable in *Y_test*. The plot shows a scatter plot of the test data and the predicted data on the same axes for each feature of *X_test*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prediction vs the test data\n",
        "with torch.no_grad():\n",
        "    Y_pred = nn_model(X_test)\n",
        "\n",
        "for i in range(Y_test.shape[-1]):\n",
        "    fig = plt.figure(figsize=(21, 9))\n",
        "    plt.title('Testcase dataset pca resampled')\n",
        "    for alpha in range(X_test.shape[-1]):\n",
        "        ax = fig.add_subplot(2, 2, alpha+1)\n",
        "        ax.scatter(X_test[:,alpha].numpy(), Y_test[:,i].numpy(),label = 'test')\n",
        "        ax.scatter(X_test[:,alpha].numpy(), Y_pred[:,i].numpy(),label = 'predicted')\n",
        "        plt.legend()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Surrogate Model\n",
        "We initialize a surrogate model using *Gaussian Processes* (GP) to predict the output values given a set of input features. We use the botorch library to create a `SingleTaskGP` model for each output dimension in *Y* and then combine them into a `ModelListGP`. The *likelihood* of the GP is set to Gaussian with a specified noise level. We also define a function to initialize the GP model by taking input features *X*, output values *Y*, and *noise level* as inputs. The function returns the marginal log-likelihood and the initialized GP model. We also add the option to load a saved state dictionary of the GP model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from botorch.models import SingleTaskGP, FixedNoiseGP, ModelListGP\n",
        "from gpytorch.mlls.exact_marginal_log_likelihood import ExactMarginalLogLikelihood\n",
        "from gpytorch.mlls.sum_marginal_log_likelihood import SumMarginalLogLikelihood\n",
        "\n",
        "# define a function to initialize the Gaussian Process model\n",
        "def initialize_model(X, Y, noise, state_dict=None):\n",
        "  # move X and Y to the device\n",
        "  train_x, train_obj = X.to(device), Y.to(device)\n",
        "  # create an empty list of models\n",
        "  models = []\n",
        "  # define a Gaussian likelihood with specified noise level\n",
        "  likelihood = gpytorch.likelihoods.GaussianLikelihood(noise=noise)#, learn_additional_noise=False)\n",
        "  \n",
        "  # loop over each output dimension in Y\n",
        "  for i in range(train_obj.shape[-1]):\n",
        "      train_y = train_obj[..., i : i + 1]\n",
        "      # create a single task Gaussian Process for each output dimension\n",
        "      models.append(\n",
        "          SingleTaskGP(\n",
        "              train_x, train_y\n",
        "          )\n",
        "      )\n",
        "  # create a model list GP by passing all the created SingleTaskGP models\n",
        "  model = ModelListGP(*models)\n",
        "  # define the marginal log-likelihood as sum of marginal log-likelihoods of all the models\n",
        "  mll = SumMarginalLogLikelihood(model.likelihood, model)\n",
        "\n",
        "  #mll = ExactMarginalLogLikelihood(likelihood, single_model) # OTHER LIKELIHOOD ?\n",
        "  ''' load state dict if it is passed '''\n",
        "  if state_dict is not None:\n",
        "    model.load_state_dict(state_dict)\n",
        "  return mll, model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define a helper function that performs the essential BO step\n",
        "We define a helper function for *Bayesian Optimization* (BO) by performing the essential BO step. The function uses the botorch library for optimizing the acquisition function. \n",
        "\n",
        "The function takes an acquisition function as an argument and returns a new candidate point that maximizes the acquisition function. \n",
        "\n",
        "The acquisition function is optimized using multiple restarts and raw samples, and the function is constrained to a set of bounds. \n",
        "\n",
        "Finally, the predicted optimal objective value is obtained by evaluating the neural network model at the candidate point. \n",
        "\n",
        "The parameters `NUM_POINTS`, `NUM_RESTARTS`, `RAW_SAMPLES`, and `NUM_INITIAL_POINTS` are defined at the beginning of the code to control the optimization parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hTMJg2RrUs4Z"
      },
      "outputs": [],
      "source": [
        "from botorch.optim import optimize_acqf\n",
        "from botorch.utils.sampling import manual_seed\n",
        "\n",
        "NUM_POINTS = 1 if not SMOKE_TEST else 1\n",
        "NUM_RESTARTS = 10 if not SMOKE_TEST else 2\n",
        "RAW_SAMPLES = 512 if not SMOKE_TEST else 32\n",
        "NUM_INITIAL_POINTS = X_lofi.shape[0]\n",
        "\n",
        "def opt_acq_and_get_next_obj(acqf):\n",
        "  \"\"\"Optimizes the acquisition function, and returns a new candidate.\"\"\"\n",
        "  with manual_seed(1234):\n",
        "    candidates, acq_value = optimize_acqf(\n",
        "        acq_function=acqf, \n",
        "        bounds=bounds,\n",
        "        q=NUM_POINTS,\n",
        "        num_restarts=NUM_RESTARTS,\n",
        "        raw_samples=RAW_SAMPLES,\n",
        "        options={\"batch_limit\": 10, \"maxiter\": 200},\n",
        "    )\n",
        "  ''' get a new obj '''\n",
        "  with torch.no_grad():\n",
        "    predicted_optimal_obj = nn_model(candidates)\n",
        "  return candidates, predicted_optimal_obj"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Perform a few steps of MFBO\n",
        "We performs a few steps of *Multi-Fidelity Bayesian optimization*. The implementation involves several packages such as `botorch.acquisition`, `botorch.fit`, `botorch.sampling`, etc. The initialize_model function initializes the *Gaussian Process* (GP) model for each acquisition function to optimize. The function also applies the noise model to the input and output, i.e., *X_lofi* and *Y_lofi*.\n",
        "\n",
        "The objective of this implementation is to iterate until it converges on accuracy test with test data (NN). It includes different acquisition functions such as *Upper Confidence Bound* (UCB), *Probability of Improvement* (PI), *Expected Improvement* (EI), *q-Expected Improvement* (qEI), *q-Noisy Expected Improvement* (qNEI), and *q-Knowledge Gradient* (qKG).\n",
        "\n",
        "The implementation fits the models, defines the qEI and qNEI acquisition modules using a *QMC sampler*, and uses the best observed noisy values as an approximation. The implementation also uses `fit_gpytorch_mll` function to optimize the *marginal likelihood* of the GP model.\n",
        "\n",
        "For each acquisition function, the implementation applies its corresponding acquisition module to maximize the acquisition function. It uses optimize_acqf function to optimize the acquisition function. The implementation sets the bounds and the number of iterations to perform the optimization. Finally, the acquisition function with the maximum value is used as the next candidate for the high-fidelity input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iter 1/32\n",
            "Iter 2/32\n",
            "Iter 3/32\n",
            "Iter 4/32\n",
            "Iter 5/32\n",
            "Iter 6/32\n",
            "Iter 7/32\n",
            "Iter 8/32\n",
            "Iter 9/32\n",
            "Iter 10/32\n",
            "Iter 11/32\n",
            "Iter 12/32\n",
            "Iter 13/32\n",
            "Iter 14/32\n",
            "Iter 15/32\n",
            "Iter 16/32\n",
            "Iter 17/32\n",
            "Iter 18/32\n",
            "Iter 19/32\n",
            "Iter 20/32\n",
            "Iter 21/32\n",
            "Iter 22/32\n",
            "Iter 23/32\n",
            "Iter 24/32\n",
            "Iter 25/32\n",
            "Iter 26/32\n",
            "Iter 27/32\n",
            "Iter 28/32\n",
            "Iter 29/32\n",
            "Iter 30/32\n",
            "Iter 31/32\n",
            "Iter 32/32\n"
          ]
        }
      ],
      "source": [
        "from botorch.acquisition import (UpperConfidenceBound,\n",
        "                                 ProbabilityOfImprovement,\n",
        "                                 NoisyExpectedImprovement,\n",
        "                                 ExpectedImprovement,\n",
        "                                 qKnowledgeGradient, # too much computational expensive wrt the provided performance/accuracy\n",
        "                                 PosteriorMean)\n",
        "from botorch.acquisition.monte_carlo import (qExpectedImprovement, \n",
        "                                             qNoisyExpectedImprovement)\n",
        "from botorch.acquisition.objective import ScalarizedPosteriorTransform\n",
        "from botorch.fit import fit_gpytorch_mll\n",
        "from botorch.sampling import SobolQMCNormalSampler\n",
        "import warnings\n",
        "#warnings.filterwarnings(\"ignore\", category=NumericalWarning)\n",
        "# single obj does not display NumericalWarning\n",
        "\n",
        "N_ITER = 32 if not SMOKE_TEST else 1 # 32 iterations --> 4.12mis\n",
        "NUM_FANTASIES = 128 if not SMOKE_TEST else 4\n",
        "MC_SAMPLES = 256 if not SMOKE_TEST else 32\n",
        "\n",
        "X_lofi = X_lofi[0:NUM_INITIAL_POINTS]\n",
        "Y_lofi = Y_lofi[0:NUM_INITIAL_POINTS]\n",
        "''' ITERATE UNTIL CONVERGE ON ACCURACY TEST WITH TEST DATA (NN)'''\n",
        "\n",
        "X_lofi_UCB,  Y_lofi_UCB  = X_lofi, Y_lofi\n",
        "X_lofi_PI,   Y_lofi_PI   = X_lofi, Y_lofi\n",
        "X_lofi_EI,   Y_lofi_EI   = X_lofi, Y_lofi\n",
        "X_lofi_qEI,  Y_lofi_qEI  = X_lofi, Y_lofi\n",
        "X_lofi_qNEI, Y_lofi_qNEI = X_lofi, Y_lofi\n",
        "'''X_lofi_qKG,  qKG_Y_lofi  = X_lofi, Y_lofi\n",
        "X_lofi_qKGp, qKGp_Y_lofi = X_lofi, Y_lofi'''\n",
        "\n",
        "mll,      gp_model      = initialize_model(X_lofi,      Y_lofi,      noise)\n",
        "mll_UCB,  gp_model_UCB  = initialize_model(X_lofi_UCB,  Y_lofi_UCB,  noise)\n",
        "mll_PI,   gp_model_PI   = initialize_model(X_lofi_PI,   Y_lofi_PI,   noise)\n",
        "mll_EI,   gp_model_EI   = initialize_model(X_lofi_EI,   Y_lofi_EI,   noise)\n",
        "mll_qEI,  gp_model_qEI  = initialize_model(X_lofi_qEI,  Y_lofi_qEI,  noise)\n",
        "mll_qNEI, gp_model_qNEI = initialize_model(X_lofi_qNEI, Y_lofi_qNEI, noise)\n",
        "'''mll_qKG,  gp_model_qKG  = initialize_model(X_lofi,  Y_lofi,  noise)\n",
        "mll_qKGp, gp_model_qKGp = initialize_model(X_lofi, Y_lofi, noise)'''\n",
        "\n",
        "for iter in range(N_ITER):\n",
        "\n",
        "     ''' fit the models '''\n",
        "     fit_gpytorch_mll(mll)\n",
        "     fit_gpytorch_mll(mll_UCB)\n",
        "     fit_gpytorch_mll(mll_PI)\n",
        "     fit_gpytorch_mll(mll_EI)\n",
        "     fit_gpytorch_mll(mll_qEI)\n",
        "     fit_gpytorch_mll(mll_qNEI)\n",
        "\n",
        "     ''' define the qEI and qNEI acquisition modules using a QMC sampler '''\n",
        "     sampler = SobolQMCNormalSampler(sample_shape=torch.Size([MC_SAMPLES]))\n",
        "     \n",
        "     pt = ScalarizedPosteriorTransform(weights=torch.ones(Y_lofi.shape[-1])*(1/Y_lofi.shape[-1]))\n",
        "\n",
        "     ''' for best_f, we use the best observed noisy values as an approximation '''\n",
        "     UCB  = UpperConfidenceBound(\n",
        "          model = gp_model_UCB, \n",
        "          beta=0.3,\n",
        "          posterior_transform=pt,\n",
        "     )\n",
        "     PI  = ProbabilityOfImprovement( \n",
        "          model = gp_model_PI, \n",
        "          best_f = Y_lofi_PI.max(),\n",
        "          posterior_transform=pt,\n",
        "          )\n",
        "     EI  = ExpectedImprovement( \n",
        "          model = gp_model_EI, \n",
        "          best_f = Y_lofi_EI.max() ,\n",
        "          posterior_transform=pt,\n",
        "     )\n",
        "     qEI  = qExpectedImprovement( \n",
        "          model = gp_model_qEI, \n",
        "          best_f = Y_lofi_qEI.max(), \n",
        "          sampler=sampler,\n",
        "          posterior_transform=pt,\n",
        "     )\n",
        "     qNEI  = qNoisyExpectedImprovement(\n",
        "          model = gp_model_qNEI, \n",
        "          X_baseline = X_lofi_qNEI, \n",
        "          sampler=sampler,\n",
        "          posterior_transform=pt,\n",
        "     )\n",
        "     '''qKG  = qKnowledgeGradient(\n",
        "          model = gp_model_qKG, \n",
        "          num_fantasies=NUM_FANTASIES\n",
        "     )\n",
        "     qKG_proper  = qKnowledgeGradient(\n",
        "          model = gp_model_qKGp, \n",
        "          num_fantasies=NUM_FANTASIES\n",
        "     )\n",
        "     argmax_pmean, max_pmean = optimize_acqf(\n",
        "      acq_function=PosteriorMean(single_model), \n",
        "      bounds=bounds,\n",
        "      q=NUM_POINTS,\n",
        "      num_restarts=10 if not SMOKE_TEST else 2,\n",
        "      raw_samples=1024 if not SMOKE_TEST else 4,\n",
        "     )\n",
        "     acqf_name = qKnowledgeGradient(\n",
        "      single_model,\n",
        "      num_fantasies=NUM_FANTASIES,\n",
        "      sampler=qKG.sampler,\n",
        "      current_value=max_pmean,\n",
        "    )'''\n",
        "     ''' optimize and get new observation '''\n",
        "     new_X_UCB,  predicted_optimal_obj_UCB  = opt_acq_and_get_next_obj(UCB)  \n",
        "     new_X_PI,   predicted_optimal_obj_PI   = opt_acq_and_get_next_obj(PI)  \n",
        "     new_X_EI,   predicted_optimal_obj_EI   = opt_acq_and_get_next_obj(EI)  \n",
        "     new_X_qEI,  predicted_optimal_obj_qEI  = opt_acq_and_get_next_obj(qEI)  \n",
        "     new_X_qNEI, predicted_optimal_obj_qNEI = opt_acq_and_get_next_obj(qNEI)  \n",
        "     new_X = (bounds[1] - bounds[0]) * torch.rand(new_X_qNEI.shape) + bounds[0]\n",
        "     with torch.no_grad(): predicted_obj = nn_model(new_X) + NOISE_LEVEL * torch.randn(1)\n",
        "\n",
        "     print(f\"Iter {iter+1}/{N_ITER}\")# - next points: {new_X}\\n pred objectives: {predicted_optimal_obj}\")\n",
        "\n",
        "     ''' update training points '''\n",
        "     X_lofi      = torch.cat([X_lofi,      new_X ])\n",
        "     X_lofi_UCB  = torch.cat([X_lofi_UCB,  new_X_UCB ])\n",
        "     X_lofi_PI   = torch.cat([X_lofi_PI,   new_X_PI ])\n",
        "     X_lofi_EI   = torch.cat([X_lofi_EI,   new_X_EI ])\n",
        "     X_lofi_qEI  = torch.cat([X_lofi_qEI,  new_X_qEI ])\n",
        "     X_lofi_qNEI = torch.cat([X_lofi_qNEI, new_X_qNEI ])\n",
        "\n",
        "     Y_lofi      = torch.cat([Y_lofi,      predicted_obj])\n",
        "     Y_lofi_UCB  = torch.cat([Y_lofi_UCB,  predicted_optimal_obj_UCB])\n",
        "     Y_lofi_PI   = torch.cat([Y_lofi_PI,   predicted_optimal_obj_PI])\n",
        "     Y_lofi_EI   = torch.cat([Y_lofi_EI,   predicted_optimal_obj_EI])\n",
        "     Y_lofi_qEI  = torch.cat([Y_lofi_qEI,  predicted_optimal_obj_qEI])\n",
        "     Y_lofi_qNEI = torch.cat([Y_lofi_qNEI, predicted_optimal_obj_qNEI])\n",
        "\n",
        "     noise  = torch.cat([noise, 1e-3*torch.ones(Y_lofi_qNEI.shape[0]) ])\n",
        "\n",
        "     mll,      gp_model      = initialize_model(X_lofi,      Y_lofi,      noise, gp_model.state_dict())\n",
        "     mll_UCB,  gp_model_UCB  = initialize_model(X_lofi_UCB,  Y_lofi_UCB,  noise, gp_model_UCB.state_dict())\n",
        "     mll_PI,   gp_model_PI   = initialize_model(X_lofi_PI,   Y_lofi_PI,   noise, gp_model_PI.state_dict())\n",
        "     mll_EI,   gp_model_EI   = initialize_model(X_lofi_EI,   Y_lofi_EI,   noise, gp_model_EI.state_dict())\n",
        "     mll_qEI,  gp_model_qEI  = initialize_model(X_lofi_qEI,  Y_lofi_qEI,  noise, gp_model_qEI.state_dict())\n",
        "     mll_qNEI, gp_model_qNEI = initialize_model(X_lofi_qNEI, Y_lofi_qNEI, noise, gp_model_qNEI.state_dict())\n",
        "     '''mll_qKG,  gp_model_qKG  = initialize_model(qKG_X_lofi,  qKG_Y_lofi,  noise)\n",
        "     mll_qKGp, gp_model_qKGp = initialize_model(qKGp_X_lofi, qKGp_Y_lofi, noise)'''\n",
        "\n",
        "# NumericalWarning for which acqf ?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display the optimal points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_mfbo(acqf=''):\n",
        "    x = X_lofi.shape[-1]\n",
        "    y = Y_lofi.shape[-1]\n",
        "    if acqf != '': _acqf = f'_{acqf}'\n",
        "    else: _acqf = acqf\n",
        "    fig = plt.figure(figsize=(21, 9))\n",
        "    plt.title(f'{acqf}')\n",
        "    for i in range(y):\n",
        "        for alpha in range(x):\n",
        "            ax = fig.add_subplot(int(y*x/2), int(x/2), alpha+x*i+1)\n",
        "            ax.scatter(X_hifi[:,alpha].numpy(), Y_hifi[:,i].numpy(),label = 'target')\n",
        "            #ax.scatter(X_lofi[:,alpha].numpy(), Y_lofi[:,i].numpy(),label = 'predicted')\n",
        "            #ax.scatter(X_lofi_UCB[:,alpha].numpy(), Y_lofi_UCB[:,i].numpy(),label = 'pred UCB')\n",
        "            #ax.scatter(X_lofi_PI[ :,alpha].numpy(), Y_lofi_PI[ :,i].numpy(),label = 'pred PI')\n",
        "            #ax.scatter(X_lofi_EI[ :,alpha].numpy(), Y_lofi_EI[ :,i].numpy(),label = 'pred EI')\n",
        "            #ax.scatter(X_lofi_qEI[:,alpha].numpy(), Y_lofi_qEI[:,i].numpy(),label = 'pred qEI')\n",
        "            ax.scatter(eval(f'X_lofi{_acqf}')[:,alpha].numpy(), eval(f'Y_lofi{_acqf}')[:,i].numpy(),label = f'pred {acqf}')\n",
        "            plt.legend()\n",
        "            plt.xlabel(f\"PCA-Wing{alpha+1}\")\n",
        "            plt.ylabel(f\"SumCX{i+1}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_mfbo('qNEI')\n",
        "plot_mfbo('UCB')\n",
        "plot_mfbo('PI')\n",
        "plot_mfbo('EI')\n",
        "plot_mfbo('qEI')\n",
        "plot_mfbo()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build a GP regression model on the optimal points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the Kernel of Gaussian Process\n",
        "class ExactGPModel(gpytorch.models.ExactGP):\n",
        "    def __init__(self,X_train, Y_train, likelihood=gpytorch.likelihoods.GaussianLikelihood()):\n",
        "        super(ExactGPModel, self).__init__(X_train, Y_train, likelihood)\n",
        "        self.mean_module = gpytorch.means.ConstantMean()\n",
        "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()) \n",
        "       \n",
        "    def forward(self, x):\n",
        "        mean_x = self.mean_module(x)\n",
        "        covar_x = self.covar_module(x)\n",
        "        return gpytorch.distributions.MultivariateNormal(mean_x,covar_x)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Helper function for training the GP regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gp_visualization(x_train,y_train,alpha=0,iter=1000,acqf='acqf'):\n",
        "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
        "    train_multi_yvar = noise**2*torch.ones(x_train.unsqueeze(-1).shape[0],1, device=device)\n",
        "    ''' Define the model '''\n",
        "    #gp_model = FixedNoiseGP(x_train.unsqueeze(-1), y_train, train_multi_yvar)\n",
        "    #gp_model = SingleTaskGP(x_train.unsqueeze(-1), y_train, likelihood = likelihood  )\n",
        "    gp_model = ExactGPModel(x_train, y_train.squeeze(), likelihood)\n",
        "    ''' Find optimal model hyperparameters '''\n",
        "    gp_model.train()\n",
        "    likelihood.train()\n",
        "    ''' Use the adam optimizer '''  # Includes GaussianLikelihood parameters\n",
        "    optimizer = torch.optim.Adam(gp_model.parameters(),lr=1e-3)\n",
        "    ''' \"Loss\" for GPs - the marginal log likelihood '''\n",
        "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp_model)\n",
        "    training_iter = iter\n",
        "    for i in range(training_iter):\n",
        "        ''' Zero gradients from previous iteration '''\n",
        "        optimizer.zero_grad()    \n",
        "        ''' Output from model '''\n",
        "        output = gp_model(x_train)\n",
        "        ''' compute loss and backprop gradients '''\n",
        "        loss = -mll(output, y_train.squeeze())\n",
        "        loss.backward()\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f'Wing{alpha+1}: {acqf} - Iter {i + 1}/{training_iter}')# - Loss: {loss.item():.5f} LenghtParam {gp_model.covar_module.base_kernel.lengthscale.detach().numpy()[0,0]:.5f}')\n",
        "        optimizer.step()\n",
        "    print('\\n')\n",
        "    return gp_model.eval(), likelihood.eval()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train the GP regression  model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wing1: rand - Iter 100/1000\n",
            "Wing1: rand - Iter 200/1000\n",
            "Wing1: rand - Iter 300/1000\n",
            "Wing1: rand - Iter 400/1000\n",
            "Wing1: rand - Iter 500/1000\n",
            "Wing1: rand - Iter 600/1000\n",
            "Wing1: rand - Iter 700/1000\n",
            "Wing1: rand - Iter 800/1000\n",
            "Wing1: rand - Iter 900/1000\n",
            "Wing1: rand - Iter 1000/1000\n",
            "\n",
            "\n",
            "Wing1: UCB - Iter 100/1000\n",
            "Wing1: UCB - Iter 200/1000\n",
            "Wing1: UCB - Iter 300/1000\n",
            "Wing1: UCB - Iter 400/1000\n",
            "Wing1: UCB - Iter 500/1000\n",
            "Wing1: UCB - Iter 600/1000\n",
            "Wing1: UCB - Iter 700/1000\n",
            "Wing1: UCB - Iter 800/1000\n",
            "Wing1: UCB - Iter 900/1000\n",
            "Wing1: UCB - Iter 1000/1000\n",
            "\n",
            "\n",
            "Wing1: PI - Iter 100/1000\n",
            "Wing1: PI - Iter 200/1000\n",
            "Wing1: PI - Iter 300/1000\n",
            "Wing1: PI - Iter 400/1000\n",
            "Wing1: PI - Iter 500/1000\n",
            "Wing1: PI - Iter 600/1000\n",
            "Wing1: PI - Iter 700/1000\n",
            "Wing1: PI - Iter 800/1000\n",
            "Wing1: PI - Iter 900/1000\n",
            "Wing1: PI - Iter 1000/1000\n",
            "\n",
            "\n",
            "Wing1: EI - Iter 100/1000\n",
            "Wing1: EI - Iter 200/1000\n",
            "Wing1: EI - Iter 300/1000\n",
            "Wing1: EI - Iter 400/1000\n",
            "Wing1: EI - Iter 500/1000\n",
            "Wing1: EI - Iter 600/1000\n",
            "Wing1: EI - Iter 700/1000\n",
            "Wing1: EI - Iter 800/1000\n",
            "Wing1: EI - Iter 900/1000\n",
            "Wing1: EI - Iter 1000/1000\n",
            "\n",
            "\n",
            "Wing1: qEI - Iter 100/1000\n",
            "Wing1: qEI - Iter 200/1000\n",
            "Wing1: qEI - Iter 300/1000\n",
            "Wing1: qEI - Iter 400/1000\n",
            "Wing1: qEI - Iter 500/1000\n",
            "Wing1: qEI - Iter 600/1000\n",
            "Wing1: qEI - Iter 700/1000\n",
            "Wing1: qEI - Iter 800/1000\n",
            "Wing1: qEI - Iter 900/1000\n",
            "Wing1: qEI - Iter 1000/1000\n",
            "\n",
            "\n",
            "Wing1: qNEI - Iter 100/1000\n",
            "Wing1: qNEI - Iter 200/1000\n",
            "Wing1: qNEI - Iter 300/1000\n",
            "Wing1: qNEI - Iter 400/1000\n",
            "Wing1: qNEI - Iter 500/1000\n",
            "Wing1: qNEI - Iter 600/1000\n",
            "Wing1: qNEI - Iter 700/1000\n",
            "Wing1: qNEI - Iter 800/1000\n",
            "Wing1: qNEI - Iter 900/1000\n",
            "Wing1: qNEI - Iter 1000/1000\n",
            "\n",
            "\n",
            "Wing2: rand - Iter 100/1000\n",
            "Wing2: rand - Iter 200/1000\n",
            "Wing2: rand - Iter 300/1000\n",
            "Wing2: rand - Iter 400/1000\n",
            "Wing2: rand - Iter 500/1000\n",
            "Wing2: rand - Iter 600/1000\n",
            "Wing2: rand - Iter 700/1000\n",
            "Wing2: rand - Iter 800/1000\n",
            "Wing2: rand - Iter 900/1000\n",
            "Wing2: rand - Iter 1000/1000\n",
            "\n",
            "\n",
            "Wing2: UCB - Iter 100/1000\n",
            "Wing2: UCB - Iter 200/1000\n",
            "Wing2: UCB - Iter 300/1000\n",
            "Wing2: UCB - Iter 400/1000\n",
            "Wing2: UCB - Iter 500/1000\n",
            "Wing2: UCB - Iter 600/1000\n",
            "Wing2: UCB - Iter 700/1000\n",
            "Wing2: UCB - Iter 800/1000\n",
            "Wing2: UCB - Iter 900/1000\n",
            "Wing2: UCB - Iter 1000/1000\n",
            "\n",
            "\n",
            "Wing2: PI - Iter 100/1000\n",
            "Wing2: PI - Iter 200/1000\n",
            "Wing2: PI - Iter 300/1000\n",
            "Wing2: PI - Iter 400/1000\n",
            "Wing2: PI - Iter 500/1000\n",
            "Wing2: PI - Iter 600/1000\n",
            "Wing2: PI - Iter 700/1000\n",
            "Wing2: PI - Iter 800/1000\n",
            "Wing2: PI - Iter 900/1000\n",
            "Wing2: PI - Iter 1000/1000\n",
            "\n",
            "\n",
            "Wing2: EI - Iter 100/1000\n",
            "Wing2: EI - Iter 200/1000\n",
            "Wing2: EI - Iter 300/1000\n",
            "Wing2: EI - Iter 400/1000\n",
            "Wing2: EI - Iter 500/1000\n",
            "Wing2: EI - Iter 600/1000\n",
            "Wing2: EI - Iter 700/1000\n",
            "Wing2: EI - Iter 800/1000\n",
            "Wing2: EI - Iter 900/1000\n",
            "Wing2: EI - Iter 1000/1000\n",
            "\n",
            "\n",
            "Wing2: qEI - Iter 100/1000\n",
            "Wing2: qEI - Iter 200/1000\n",
            "Wing2: qEI - Iter 300/1000\n",
            "Wing2: qEI - Iter 400/1000\n",
            "Wing2: qEI - Iter 500/1000\n",
            "Wing2: qEI - Iter 600/1000\n",
            "Wing2: qEI - Iter 700/1000\n",
            "Wing2: qEI - Iter 800/1000\n",
            "Wing2: qEI - Iter 900/1000\n",
            "Wing2: qEI - Iter 1000/1000\n",
            "\n",
            "\n",
            "Wing2: qNEI - Iter 100/1000\n",
            "Wing2: qNEI - Iter 200/1000\n",
            "Wing2: qNEI - Iter 300/1000\n",
            "Wing2: qNEI - Iter 400/1000\n",
            "Wing2: qNEI - Iter 500/1000\n",
            "Wing2: qNEI - Iter 600/1000\n",
            "Wing2: qNEI - Iter 700/1000\n",
            "Wing2: qNEI - Iter 800/1000\n",
            "Wing2: qNEI - Iter 900/1000\n",
            "Wing2: qNEI - Iter 1000/1000\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "TRAIN_ITER = 1000 if not SMOKE_TEST else 4\n",
        "likelihood_list,      gp_model_list      = [],[]\n",
        "likelihood_UCB_list,  gp_model_UCB_list  = [],[]\n",
        "likelihood_PI_list,   gp_model_PI_list   = [],[]\n",
        "likelihood_EI_list,   gp_model_EI_list   = [],[]\n",
        "likelihood_qEI_list,  gp_model_qEI_list  = [],[]\n",
        "likelihood_qNEI_list, gp_model_qNEI_list = [],[]\n",
        "for alpha in range(2):#X_lofi.shape[-1]):\n",
        "    ''' Fit the model '''\n",
        "    likelihood,      gp_model      = gp_visualization(X_lofi[NUM_INITIAL_POINTS:,alpha],     Y_lofi[NUM_INITIAL_POINTS:,0],     alpha,iter=TRAIN_ITER, acqf='rand')\n",
        "    likelihood_UCB,  gp_model_UCB  = gp_visualization(X_lofi_UCB[NUM_INITIAL_POINTS:,alpha], Y_lofi_UCB[NUM_INITIAL_POINTS:,0], alpha,iter=TRAIN_ITER, acqf='UCB')\n",
        "    likelihood_PI,   gp_model_PI   = gp_visualization(X_lofi_PI[NUM_INITIAL_POINTS:,alpha],  Y_lofi_PI[NUM_INITIAL_POINTS:,0],  alpha,iter=TRAIN_ITER, acqf='PI')\n",
        "    likelihood_EI,   gp_model_EI   = gp_visualization(X_lofi_EI[NUM_INITIAL_POINTS:,alpha],  Y_lofi_EI[NUM_INITIAL_POINTS:,0],  alpha,iter=TRAIN_ITER, acqf='EI')\n",
        "    likelihood_qEI,  gp_model_qEI  = gp_visualization(X_lofi_qEI[NUM_INITIAL_POINTS:,alpha], Y_lofi_qEI[NUM_INITIAL_POINTS:,0], alpha,iter=TRAIN_ITER, acqf='qEI')\n",
        "    likelihood_qNEI, gp_model_qNEI = gp_visualization(X_lofi_qNEI[NUM_INITIAL_POINTS:,alpha],Y_lofi_qNEI[NUM_INITIAL_POINTS:,0],alpha,iter=TRAIN_ITER, acqf='qNEI')\n",
        "    likelihood_list.append(likelihood),           gp_model_list.append(gp_model)\n",
        "    likelihood_UCB_list.append(likelihood_UCB),   gp_model_UCB_list.append(gp_model_UCB)\n",
        "    likelihood_PI_list.append(likelihood_PI),     gp_model_PI_list.append(gp_model_PI)\n",
        "    likelihood_EI_list.append(likelihood_EI),     gp_model_EI_list.append(gp_model_EI)\n",
        "    likelihood_qEI_list.append(likelihood_qEI),   gp_model_qEI_list.append(gp_model_qEI)\n",
        "    likelihood_qNEI_list.append(likelihood_qNEI), gp_model_qNEI_list.append(gp_model_qNEI)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Helper function to display the GP regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_gp(gp_model_acqf,acqf=''):\n",
        "    x = X_lofi.shape[-1]\n",
        "    y = Y_lofi.shape[-1]\n",
        "    if acqf != '': \n",
        "        _acqf = f'_{acqf}'\n",
        "    else: \n",
        "        _acqf, acqf = acqf, 'rand'\n",
        "    fig = plt.figure(figsize=(21, 9))\n",
        "    plt.title(f'{acqf}')\n",
        "    for i in range(y):\n",
        "        for alpha in range(x):\n",
        "\n",
        "            with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
        "                test_x = torch.linspace(X_hifi[:,alpha].min(), X_hifi[:,alpha].max(), 100)\n",
        "                # Make predictions by feeding model through likelihood / gp_model\n",
        "                observed_pred_acqf = gp_model_acqf((test_x ))  \n",
        "                # Initialize plot \n",
        "                ax = fig.add_subplot(int(y*x/2), int(x/2), alpha+x*i+1)     \n",
        "                ax.scatter(X_test[:,alpha].numpy(), Y_test[:,-1].numpy(), alpha=0.8, label = 'Test')\n",
        "                \n",
        "                # Get upper and lower confidence bounds # Plot predictive means as blue line\n",
        "                lower, upper = observed_pred_acqf.confidence_region() \n",
        "                ax.plot(test_x.numpy(), observed_pred_acqf.mean.numpy(), label = f'Mean {acqf}')    \n",
        "                ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5, label = f'Confidence {acqf}')  \n",
        "                ax.scatter(eval(f'X_lofi{_acqf}')[:,alpha].numpy(), eval(f'Y_lofi{_acqf}')[:,-1].numpy(), s=50,label = f'Observed Data {acqf}')\n",
        "                ax.set_ylim([-4,3])\n",
        "                plt.xlabel(f\"PCA-Wing{alpha+1}\")\n",
        "                ax.legend()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display the GP regression model results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_gp(gp_model)\n",
        "plot_gp(gp_model_UCB,'UCB')\n",
        "plot_gp(gp_model_PI,'PI')\n",
        "plot_gp(gp_model_EI,'EI')\n",
        "plot_gp(gp_model_qEI,'qEI')\n",
        "plot_gp(gp_model_qNEI,'qNEI')\n",
        "\n",
        "\n",
        "plt.show()\n",
        "# ADD NON-PCA DATA (ORIGINAL DATA) PLOT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for alpha in range(X_lofi.shape[-1]):\n",
        "    likelihood, gp_model = gp_visualization(X_test[:,alpha], Y_test[:,-1].unsqueeze(0),alpha,iter=300, acqf='')\n",
        "\n",
        "\n",
        "acqf=''\n",
        "gp_model_acqf = likelihood\n",
        "x = X_lofi.shape[-1]\n",
        "y = Y_lofi.shape[-1]\n",
        "if acqf != '': \n",
        "    _acqf = f'_{acqf}'\n",
        "else: \n",
        "    _acqf, acqf = acqf, 'rand'\n",
        "fig = plt.figure(figsize=(21, 9))\n",
        "plt.title(f'{acqf}')\n",
        "for i in range(y):\n",
        "    for alpha in range(x):\n",
        "\n",
        "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
        "            test_x = torch.linspace(X_hifi[:,alpha].min(), X_hifi[:,alpha].max(), 100)\n",
        "            # Make predictions by feeding model through likelihood / gp_model\n",
        "            observed_pred_acqf = gp_model_acqf((test_x ))  \n",
        "            # Initialize plot \n",
        "            ax = fig.add_subplot(int(y*x/2), int(x/2), alpha+x*i+1)     \n",
        "            ax.scatter(X_test[:,alpha].numpy(), Y_test[:,-1].numpy(), alpha=0.8, label = 'Test')\n",
        "            \n",
        "            # Get upper and lower confidence bounds # Plot predictive means as blue line\n",
        "            lower, upper = observed_pred_acqf.confidence_region() \n",
        "            ax.plot(test_x.numpy(), observed_pred_acqf.mean.numpy(), label = f'Mean {acqf}')    \n",
        "            ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5, label = f'Confidence {acqf}')  \n",
        "            \n",
        "            ax.set_ylim([-4,3])\n",
        "            plt.xlabel(f\"PCA-Wing{alpha+1}\")\n",
        "            ax.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gp import raw_gp\n",
        "raw_gp(X_test[:,0],Y_test[:,0])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotly\n",
        "We define some helper functions for creating and updating a Plotly graph in the interactive dashboard. The update_layout_of_graph function updates the layout of the graph with the specified title, x-axis and y-axis labels, legend orientation and position, and axis line properties. The uncertainty_area_scatter function creates a scatter plot with an area filled between the upper and lower bounds. The line_scatter function creates a line plot from the specified x and y values. The test_scatter function creates a scatter plot of test data points. The dot_scatter function creates a scatter plot of observed data points. All functions return a Plotly scatter plot object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [],
      "source": [
        "import plotly.graph_objs as go\n",
        "from visualize import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for alpha in range(2):\n",
        "    test_x = torch.linspace(X_hifi[:,alpha].min(), X_hifi[:,alpha].max(), 100)\n",
        "    plot      = plot_GPR(data_x=X_lofi[NUM_INITIAL_POINTS:,alpha],      \n",
        "                         data_y=Y_lofi[NUM_INITIAL_POINTS:,-1],      \n",
        "                         x=test_x, model=likelihood_list[alpha],     legend_group=\"rand\"\n",
        "                         )\n",
        "    plot_UCB  = plot_GPR(data_x=X_lofi_UCB[NUM_INITIAL_POINTS:,alpha],  \n",
        "                         data_y=Y_lofi_UCB[NUM_INITIAL_POINTS:,0],  \n",
        "                         x=test_x, model=likelihood_UCB_list[alpha], legend_group=\"UCB\",  color = 'coral'\n",
        "                         )\n",
        "    plot_PI   = plot_GPR(data_x=X_lofi_PI[NUM_INITIAL_POINTS:,alpha],   \n",
        "                         data_y=Y_lofi_PI[NUM_INITIAL_POINTS:,0],   \n",
        "                         x=test_x, model=likelihood_PI_list[alpha],  legend_group=\"PI\",   color = 'cornflowerblue'\n",
        "                         )\n",
        "    plot_EI   = plot_GPR(data_x=X_lofi_EI[NUM_INITIAL_POINTS:,alpha],   \n",
        "                         data_y=Y_lofi_EI[NUM_INITIAL_POINTS:,0],   \n",
        "                         x=test_x, model=likelihood_EI_list[alpha],  legend_group=\"EI\",   color = 'crimson'\n",
        "                         )\n",
        "    plot_qEI  = plot_GPR(data_x=X_lofi_qEI[NUM_INITIAL_POINTS:,alpha],  \n",
        "                         data_y=Y_lofi_qEI[NUM_INITIAL_POINTS:,0],  \n",
        "                         x=test_x, model=likelihood_qEI_list[alpha], legend_group=\"qEI\",  color = 'darkblue'\n",
        "                         )\n",
        "    plot_qNEI = plot_GPR(data_x=X_lofi_qNEI[NUM_INITIAL_POINTS:,alpha], \n",
        "                         data_y=Y_lofi_qNEI[NUM_INITIAL_POINTS:,0], \n",
        "                         x=test_x, model=likelihood_qNEI_list[alpha],legend_group=\"qNEI\", color = 'darkgoldenrod'\n",
        "                         )\n",
        "    fig4 = go.Figure()\n",
        "    fig4.add_trace(go.Scatter(\n",
        "        x=X_test[:,alpha],\n",
        "        y=Y_test[:,0],\n",
        "        legendgroup=\"Test\",  \n",
        "        name=\"Test\",\n",
        "        mode=\"markers\",\n",
        "        marker=dict(color=\"burlywood\", size=7)\n",
        "    ))\n",
        "    fig4.add_traces(data=plot )\n",
        "    fig4.add_traces(data=plot_UCB )\n",
        "    fig4.add_traces(data=plot_PI )\n",
        "    fig4.add_traces(data=plot_EI )\n",
        "    fig4.add_traces(data=plot_qEI )\n",
        "    fig4.add_traces(data=plot_qNEI)\n",
        "    fig4.update_layout(\n",
        "            width=1500,\n",
        "            height=800,\n",
        "            autosize=False,\n",
        "            plot_bgcolor='rgba(0,0,0,0)',\n",
        "            title='WingPCA'+str(alpha+1)+' - GPR on Opt Points vs Test Points')\n",
        "    fig4 = update_layout_of_graph(fig=fig4,\n",
        "                              title='GPR on Opt Points vs Test Points')\n",
        "\n",
        "    fig4.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "advpy",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "31b1b49fb1d8e2ae9d1d5ed1be0687462d4838cc25d0c2b5c85836c4eda48a38"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
