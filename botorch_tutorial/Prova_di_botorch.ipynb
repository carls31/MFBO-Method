{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        },
        "id": "fJ1_RXzgs_FN",
        "outputId": "68d622d7-0e37-44af-fdd9-f4b7b3cf9d98"
      },
      "outputs": [],
      "source": [
        "#!pip install botorch\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dtype = torch.double"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YCL5yZQvTkhB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "#df = pd.read_csv('Case3_2nd_launch_WingsConvCoeffs_Info.csv', sep=\";\")\n",
        "df = pd.read_excel('Case5_1st_launch_WingsConvCoeffs_Info.xlsx')\n",
        "df_conv = df[df['Unnamed: 47'] == 'CONVERGED']\n",
        "from sklearn.model_selection import train_test_split\n",
        "hifi, lofi = train_test_split(df_conv, test_size=0.1, random_state=42 )\n",
        "\n",
        "hifi_x = hifi.alpha0.to_numpy()\n",
        "lofi_x  = lofi.alpha0.to_numpy()\n",
        "hifi_y = hifi.Cy0Mean.to_numpy()\n",
        "lofi_y  = lofi.Cy0Mean.to_numpy()\n",
        "\n",
        "# normalize features\n",
        "mean = hifi_x.mean( )\n",
        "std = hifi_x.std( ) + 1e-6 # prevent dividing by 0\n",
        "hifi_x = (hifi_x - mean) / std\n",
        "lofi_x = (lofi_x - mean) / std\n",
        "\n",
        "# normalize labels\n",
        "mean, std = hifi_y.mean(), hifi_y.std()\n",
        "hifi_y = (hifi_y - mean) / std\n",
        "lofi_y = (lofi_y - mean) / std\n",
        "\n",
        "# make continguous\n",
        "#hifi_x, hifi_y = hifi_x.contiguous(), hifi_y.contiguous()\n",
        "#test_x, test_y = test_x.contiguous(), lofi_y.contiguous()\n",
        "\n",
        "# Cast them\n",
        "X_hifi = torch.FloatTensor(hifi_x).unsqueeze(-1)\n",
        "X_lofi = torch.FloatTensor(lofi_x).unsqueeze(-1)\n",
        "Y_hifi = torch.FloatTensor(hifi_y).unsqueeze(-1)\n",
        "Y_lofi = torch.FloatTensor(lofi_y).unsqueeze(-1)\n",
        "\n",
        "\n",
        "# x_hifi = torch.tensor(df.awa, device=device, dtype=dtype).unsqueeze(-1)  \n",
        "#x_hifi = torch.tensor(df2['awa'].values).unsqueeze(-1)\n",
        "#y_hifi = torch.tensor(df2['Cy0Mean'].values).unsqueeze(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from botorch.models import SingleTaskGP\n",
        "from gpytorch.mlls.exact_marginal_log_likelihood import ExactMarginalLogLikelihood    \n",
        "from botorch.fit import fit_gpytorch_mll\n",
        "\n",
        "best_obs_value = lofi_y.max()\n",
        "bounds = torch.FloatTensor([[-1.9], [1.6]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hTMJg2RrUs4Z"
      },
      "outputs": [],
      "source": [
        "from botorch.acquisition.monte_carlo import qExpectedImprovement\n",
        "from botorch.optim import optimize_acqf\n",
        "\n",
        "def get_next_points(x_lofi, y_lofi,best_obs_value, bounds, n_points=1):\n",
        "\n",
        "  single_model = SingleTaskGP(x_lofi, y_lofi)\n",
        "  mll = ExactMarginalLogLikelihood(single_model.likelihood, single_model)\n",
        "  fit_gpytorch_mll(mll)\n",
        "\n",
        "  EI = qExpectedImprovement( model = single_model, best_f = best_obs_value )\n",
        "\n",
        "  candidates, _ = optimize_acqf(\n",
        "    acq_function = EI,\n",
        "    bounds = bounds,\n",
        "    q = n_points,\n",
        "    num_restarts= 200,\n",
        "    raw_samples = 512 )\n",
        "  return candidates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-1.7463],\n",
              "        [-1.9000]])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_next_points(X_lofi, Y_lofi, best_obs_value, bounds, n_points=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the Kernel of GP\n",
        "import gpytorch\n",
        "class ExactGPModel(gpytorch.models.ExactGP):\n",
        "    def __init__(self,X_train, Y_train,likelihood):\n",
        "        super(ExactGPModel, self).__init__(X_train, Y_train, likelihood)\n",
        "        # this serve for prior\n",
        "        #self.mean_module = gpytorch.means.ZeroMean()\n",
        "        #PeriodicKernel =  gpytorch.kernels.ScaleKernel(gpytorch.kernels.PeriodicKernel())\n",
        "        #RQKernel =  gpytorch.kernels.ScaleKernel(gpytorch.kernels.RQKernel())\n",
        "        #self.covar_module = gpytorch.kernels.ProductKernel(PeriodicKernel,RQKernel)\n",
        "        self.mean_module = gpytorch.means.ConstantMean()\n",
        "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()) \n",
        "\n",
        "       \n",
        "    def forward(self, x):\n",
        "        mean_x = self.mean_module(x)\n",
        "        covar_x = self.covar_module(x)\n",
        "        return gpytorch.distributions.MultivariateNormal(mean_x,covar_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iter 1/500 - Loss: 4.30378532409668 LenghtParam [[0.6931472]] \n",
            "Iter 2/500 - Loss: 4.30088472366333 LenghtParam [[0.6936473]] \n",
            "Iter 3/500 - Loss: 4.302562236785889 LenghtParam [[0.6941226]] \n",
            "Iter 4/500 - Loss: 4.300481796264648 LenghtParam [[0.69459796]] \n",
            "Iter 5/500 - Loss: 4.298645496368408 LenghtParam [[0.6950585]] \n",
            "Iter 6/500 - Loss: 4.3156585693359375 LenghtParam [[0.69552064]] \n",
            "Iter 7/500 - Loss: 4.298374652862549 LenghtParam [[0.69599724]] \n",
            "Iter 8/500 - Loss: 4.305403232574463 LenghtParam [[0.6964353]] \n",
            "Iter 9/500 - Loss: 4.294623374938965 LenghtParam [[0.6968891]] \n",
            "Iter 10/500 - Loss: 4.306479454040527 LenghtParam [[0.6973277]] \n",
            "Iter 11/500 - Loss: 4.301057815551758 LenghtParam [[0.69778603]] \n",
            "Iter 12/500 - Loss: 4.317960739135742 LenghtParam [[0.69825727]] \n",
            "Iter 13/500 - Loss: 4.314401626586914 LenghtParam [[0.6987272]] \n",
            "Iter 14/500 - Loss: 4.3045806884765625 LenghtParam [[0.6991919]] \n",
            "Iter 15/500 - Loss: 4.293104648590088 LenghtParam [[0.69967324]] \n",
            "Iter 16/500 - Loss: 4.306142807006836 LenghtParam [[0.7001463]] \n",
            "Iter 17/500 - Loss: 4.303548812866211 LenghtParam [[0.7006167]] \n",
            "Iter 18/500 - Loss: 4.298973083496094 LenghtParam [[0.7010724]] \n",
            "Iter 19/500 - Loss: 4.299580097198486 LenghtParam [[0.70150775]] \n",
            "Iter 20/500 - Loss: 4.297418117523193 LenghtParam [[0.70194197]] \n",
            "Iter 21/500 - Loss: 4.3081583976745605 LenghtParam [[0.7023552]] \n",
            "Iter 22/500 - Loss: 4.305415630340576 LenghtParam [[0.7027913]] \n",
            "Iter 23/500 - Loss: 4.302194595336914 LenghtParam [[0.70324236]] \n",
            "Iter 24/500 - Loss: 4.298830032348633 LenghtParam [[0.7036939]] \n",
            "Iter 25/500 - Loss: 4.303131103515625 LenghtParam [[0.7041574]] \n",
            "Iter 26/500 - Loss: 4.300032138824463 LenghtParam [[0.70461214]] \n",
            "Iter 27/500 - Loss: 4.312191486358643 LenghtParam [[0.70503384]] \n",
            "Iter 28/500 - Loss: 4.299588680267334 LenghtParam [[0.7054843]] \n",
            "Iter 29/500 - Loss: 4.291988372802734 LenghtParam [[0.7059272]] \n",
            "Iter 30/500 - Loss: 4.3075666427612305 LenghtParam [[0.7063389]] \n",
            "Iter 31/500 - Loss: 4.301061630249023 LenghtParam [[0.70679426]] \n",
            "Iter 32/500 - Loss: 4.30228853225708 LenghtParam [[0.7072181]] \n",
            "Iter 33/500 - Loss: 4.30472469329834 LenghtParam [[0.70765203]] \n",
            "Iter 34/500 - Loss: 4.305398464202881 LenghtParam [[0.7080975]] \n",
            "Iter 35/500 - Loss: 4.307071208953857 LenghtParam [[0.70854986]] \n",
            "Iter 36/500 - Loss: 4.307187557220459 LenghtParam [[0.70900905]] \n",
            "Iter 37/500 - Loss: 4.29412841796875 LenghtParam [[0.7094957]] \n",
            "Iter 38/500 - Loss: 4.30124568939209 LenghtParam [[0.7099605]] \n",
            "Iter 39/500 - Loss: 4.315454959869385 LenghtParam [[0.710429]] \n",
            "Iter 40/500 - Loss: 4.297152519226074 LenghtParam [[0.7109483]] \n",
            "Iter 41/500 - Loss: 4.305447101593018 LenghtParam [[0.71147096]] \n",
            "Iter 42/500 - Loss: 4.29991340637207 LenghtParam [[0.7119688]] \n",
            "Iter 43/500 - Loss: 4.312239646911621 LenghtParam [[0.71244824]] \n",
            "Iter 44/500 - Loss: 4.2989606857299805 LenghtParam [[0.7129347]] \n",
            "Iter 45/500 - Loss: 4.3044753074646 LenghtParam [[0.71343297]] \n",
            "Iter 46/500 - Loss: 4.300721645355225 LenghtParam [[0.71392685]] \n",
            "Iter 47/500 - Loss: 4.3093719482421875 LenghtParam [[0.71442074]] \n",
            "Iter 48/500 - Loss: 4.303802490234375 LenghtParam [[0.7149115]] \n",
            "Iter 49/500 - Loss: 4.30199670791626 LenghtParam [[0.71538645]] \n",
            "Iter 50/500 - Loss: 4.301443576812744 LenghtParam [[0.7158381]] \n",
            "Iter 51/500 - Loss: 4.302535057067871 LenghtParam [[0.71631557]] \n",
            "Iter 52/500 - Loss: 4.299056053161621 LenghtParam [[0.71681017]] \n",
            "Iter 53/500 - Loss: 4.298875331878662 LenghtParam [[0.71729976]] \n",
            "Iter 54/500 - Loss: 4.303598880767822 LenghtParam [[0.71778196]] \n",
            "Iter 55/500 - Loss: 4.307141304016113 LenghtParam [[0.7182537]] \n",
            "Iter 56/500 - Loss: 4.307801246643066 LenghtParam [[0.7187348]] \n",
            "Iter 57/500 - Loss: 4.3046793937683105 LenghtParam [[0.7192272]] \n",
            "Iter 58/500 - Loss: 4.300045013427734 LenghtParam [[0.71971494]] \n",
            "Iter 59/500 - Loss: 4.301960468292236 LenghtParam [[0.7201839]] \n",
            "Iter 60/500 - Loss: 4.303627014160156 LenghtParam [[0.72065693]] \n",
            "Iter 61/500 - Loss: 4.292144298553467 LenghtParam [[0.72113526]] \n",
            "Iter 62/500 - Loss: 4.299528121948242 LenghtParam [[0.7215893]] \n",
            "Iter 63/500 - Loss: 4.306813716888428 LenghtParam [[0.7220187]] \n",
            "Iter 64/500 - Loss: 4.297773361206055 LenghtParam [[0.72243816]] \n",
            "Iter 65/500 - Loss: 4.302101135253906 LenghtParam [[0.7228315]] \n",
            "Iter 66/500 - Loss: 4.298930644989014 LenghtParam [[0.7232521]] \n",
            "Iter 67/500 - Loss: 4.298589706420898 LenghtParam [[0.72367615]] \n",
            "Iter 68/500 - Loss: 4.299597263336182 LenghtParam [[0.72410095]] \n",
            "Iter 69/500 - Loss: 4.300325870513916 LenghtParam [[0.72451156]] \n",
            "Iter 70/500 - Loss: 4.2982707023620605 LenghtParam [[0.72490627]] \n",
            "Iter 71/500 - Loss: 4.294159889221191 LenghtParam [[0.7253042]] \n",
            "Iter 72/500 - Loss: 4.303876876831055 LenghtParam [[0.7256904]] \n",
            "Iter 73/500 - Loss: 4.308661937713623 LenghtParam [[0.7260637]] \n",
            "Iter 74/500 - Loss: 4.306739330291748 LenghtParam [[0.72644836]] \n",
            "Iter 75/500 - Loss: 4.293955326080322 LenghtParam [[0.7269011]] \n",
            "Iter 76/500 - Loss: 4.305786609649658 LenghtParam [[0.7273439]] \n",
            "Iter 77/500 - Loss: 4.305718421936035 LenghtParam [[0.727787]] \n",
            "Iter 78/500 - Loss: 4.2986159324646 LenghtParam [[0.7282246]] \n",
            "Iter 79/500 - Loss: 4.306163787841797 LenghtParam [[0.7286711]] \n",
            "Iter 80/500 - Loss: 4.305224418640137 LenghtParam [[0.7291259]] \n",
            "Iter 81/500 - Loss: 4.296720027923584 LenghtParam [[0.7295907]] \n",
            "Iter 82/500 - Loss: 4.301419734954834 LenghtParam [[0.73004335]] \n",
            "Iter 83/500 - Loss: 4.3034515380859375 LenghtParam [[0.73047274]] \n",
            "Iter 84/500 - Loss: 4.3048176765441895 LenghtParam [[0.7309048]] \n",
            "Iter 85/500 - Loss: 4.309124946594238 LenghtParam [[0.7313572]] \n",
            "Iter 86/500 - Loss: 4.298285007476807 LenghtParam [[0.73179984]] \n",
            "Iter 87/500 - Loss: 4.301271915435791 LenghtParam [[0.7322444]] \n",
            "Iter 88/500 - Loss: 4.301474571228027 LenghtParam [[0.7326874]] \n",
            "Iter 89/500 - Loss: 4.301226615905762 LenghtParam [[0.73317003]] \n",
            "Iter 90/500 - Loss: 4.296544551849365 LenghtParam [[0.7336839]] \n",
            "Iter 91/500 - Loss: 4.2974629402160645 LenghtParam [[0.73418194]] \n",
            "Iter 92/500 - Loss: 4.30353307723999 LenghtParam [[0.73468095]] \n",
            "Iter 93/500 - Loss: 4.309774398803711 LenghtParam [[0.7351781]] \n",
            "Iter 94/500 - Loss: 4.301772117614746 LenghtParam [[0.7356883]] \n",
            "Iter 95/500 - Loss: 4.30194616317749 LenghtParam [[0.73618925]] \n",
            "Iter 96/500 - Loss: 4.297194004058838 LenghtParam [[0.73668736]] \n",
            "Iter 97/500 - Loss: 4.295149326324463 LenghtParam [[0.73719394]] \n",
            "Iter 98/500 - Loss: 4.307442665100098 LenghtParam [[0.73767734]] \n",
            "Iter 99/500 - Loss: 4.304825305938721 LenghtParam [[0.7381865]] \n",
            "Iter 100/500 - Loss: 4.306797981262207 LenghtParam [[0.73871094]] \n",
            "Iter 101/500 - Loss: 4.303623199462891 LenghtParam [[0.7392404]] \n",
            "Iter 102/500 - Loss: 4.3061628341674805 LenghtParam [[0.7397659]] \n",
            "Iter 103/500 - Loss: 4.301209926605225 LenghtParam [[0.7402791]] \n",
            "Iter 104/500 - Loss: 4.310888767242432 LenghtParam [[0.74078596]] \n",
            "Iter 105/500 - Loss: 4.309252738952637 LenghtParam [[0.7412851]] \n",
            "Iter 106/500 - Loss: 4.302028179168701 LenghtParam [[0.7418101]] \n",
            "Iter 107/500 - Loss: 4.298447132110596 LenghtParam [[0.74232155]] \n",
            "Iter 108/500 - Loss: 4.301655292510986 LenghtParam [[0.7428096]] \n",
            "Iter 109/500 - Loss: 4.294075012207031 LenghtParam [[0.74332744]] \n",
            "Iter 110/500 - Loss: 4.312621593475342 LenghtParam [[0.74382406]] \n",
            "Iter 111/500 - Loss: 4.307895660400391 LenghtParam [[0.74434316]] \n",
            "Iter 112/500 - Loss: 4.303921222686768 LenghtParam [[0.7449148]] \n",
            "Iter 113/500 - Loss: 4.30436897277832 LenghtParam [[0.74546134]] \n",
            "Iter 114/500 - Loss: 4.3022685050964355 LenghtParam [[0.74602973]] \n",
            "Iter 115/500 - Loss: 4.301935195922852 LenghtParam [[0.7465621]] \n",
            "Iter 116/500 - Loss: 4.295944690704346 LenghtParam [[0.7471064]] \n",
            "Iter 117/500 - Loss: 4.301112174987793 LenghtParam [[0.7476327]] \n",
            "Iter 118/500 - Loss: 4.303417205810547 LenghtParam [[0.748136]] \n",
            "Iter 119/500 - Loss: 4.312833309173584 LenghtParam [[0.74863166]] \n",
            "Iter 120/500 - Loss: 4.30859375 LenghtParam [[0.7491234]] \n",
            "Iter 121/500 - Loss: 4.301639080047607 LenghtParam [[0.74962366]] \n",
            "Iter 122/500 - Loss: 4.302783966064453 LenghtParam [[0.75012803]] \n",
            "Iter 123/500 - Loss: 4.302621841430664 LenghtParam [[0.75066215]] \n",
            "Iter 124/500 - Loss: 4.303206920623779 LenghtParam [[0.75116354]] \n",
            "Iter 125/500 - Loss: 4.294109344482422 LenghtParam [[0.7516717]] \n",
            "Iter 126/500 - Loss: 4.299708366394043 LenghtParam [[0.7521486]] \n",
            "Iter 127/500 - Loss: 4.303189277648926 LenghtParam [[0.75260645]] \n",
            "Iter 128/500 - Loss: 4.309976577758789 LenghtParam [[0.75305194]] \n",
            "Iter 129/500 - Loss: 4.296128749847412 LenghtParam [[0.75349295]] \n",
            "Iter 130/500 - Loss: 4.307425022125244 LenghtParam [[0.753935]] \n",
            "Iter 131/500 - Loss: 4.297342300415039 LenghtParam [[0.754393]] \n",
            "Iter 132/500 - Loss: 4.304296493530273 LenghtParam [[0.75483584]] \n",
            "Iter 133/500 - Loss: 4.298135280609131 LenghtParam [[0.75529975]] \n",
            "Iter 134/500 - Loss: 4.303429126739502 LenghtParam [[0.7557577]] \n",
            "Iter 135/500 - Loss: 4.3052144050598145 LenghtParam [[0.75619376]] \n",
            "Iter 136/500 - Loss: 4.3033928871154785 LenghtParam [[0.75664943]] \n",
            "Iter 137/500 - Loss: 4.304905891418457 LenghtParam [[0.75710607]] \n",
            "Iter 138/500 - Loss: 4.300019264221191 LenghtParam [[0.7575515]] \n",
            "Iter 139/500 - Loss: 4.30776309967041 LenghtParam [[0.7580083]] \n",
            "Iter 140/500 - Loss: 4.307388782501221 LenghtParam [[0.7584867]] \n",
            "Iter 141/500 - Loss: 4.302645683288574 LenghtParam [[0.75899965]] \n",
            "Iter 142/500 - Loss: 4.300375938415527 LenghtParam [[0.7594839]] \n",
            "Iter 143/500 - Loss: 4.301791191101074 LenghtParam [[0.75995]] \n",
            "Iter 144/500 - Loss: 4.300207138061523 LenghtParam [[0.76042193]] \n",
            "Iter 145/500 - Loss: 4.296269416809082 LenghtParam [[0.7608766]] \n",
            "Iter 146/500 - Loss: 4.304012298583984 LenghtParam [[0.76132697]] \n",
            "Iter 147/500 - Loss: 4.293949604034424 LenghtParam [[0.76180315]] \n",
            "Iter 148/500 - Loss: 4.302885055541992 LenghtParam [[0.76226944]] \n",
            "Iter 149/500 - Loss: 4.2964067459106445 LenghtParam [[0.7627651]] \n",
            "Iter 150/500 - Loss: 4.2991790771484375 LenghtParam [[0.76326555]] \n",
            "Iter 151/500 - Loss: 4.296528339385986 LenghtParam [[0.7637601]] \n",
            "Iter 152/500 - Loss: 4.30587911605835 LenghtParam [[0.7642181]] \n",
            "Iter 153/500 - Loss: 4.30906867980957 LenghtParam [[0.76471925]] \n",
            "Iter 154/500 - Loss: 4.306972503662109 LenghtParam [[0.76524544]] \n",
            "Iter 155/500 - Loss: 4.302430629730225 LenghtParam [[0.76575834]] \n",
            "Iter 156/500 - Loss: 4.300806522369385 LenghtParam [[0.7662722]] \n",
            "Iter 157/500 - Loss: 4.307453632354736 LenghtParam [[0.76678896]] \n",
            "Iter 158/500 - Loss: 4.299968242645264 LenghtParam [[0.76733625]] \n",
            "Iter 159/500 - Loss: 4.292946815490723 LenghtParam [[0.7678692]] \n",
            "Iter 160/500 - Loss: 4.307164669036865 LenghtParam [[0.7683624]] \n",
            "Iter 161/500 - Loss: 4.3032732009887695 LenghtParam [[0.7688478]] \n",
            "Iter 162/500 - Loss: 4.3079328536987305 LenghtParam [[0.7693477]] \n",
            "Iter 163/500 - Loss: 4.304944038391113 LenghtParam [[0.76981723]] \n",
            "Iter 164/500 - Loss: 4.297686576843262 LenghtParam [[0.77031046]] \n",
            "Iter 165/500 - Loss: 4.3044610023498535 LenghtParam [[0.7707781]] \n",
            "Iter 166/500 - Loss: 4.309511184692383 LenghtParam [[0.7712612]] \n",
            "Iter 167/500 - Loss: 4.305253505706787 LenghtParam [[0.7717278]] \n",
            "Iter 168/500 - Loss: 4.296075820922852 LenghtParam [[0.77217233]] \n",
            "Iter 169/500 - Loss: 4.309267520904541 LenghtParam [[0.7726018]] \n",
            "Iter 170/500 - Loss: 4.3037872314453125 LenghtParam [[0.7730989]] \n",
            "Iter 171/500 - Loss: 4.293230056762695 LenghtParam [[0.77359056]] \n",
            "Iter 172/500 - Loss: 4.307731628417969 LenghtParam [[0.77403206]] \n",
            "Iter 173/500 - Loss: 4.304758071899414 LenghtParam [[0.7744765]] \n",
            "Iter 174/500 - Loss: 4.30950403213501 LenghtParam [[0.7749225]] \n",
            "Iter 175/500 - Loss: 4.2984442710876465 LenghtParam [[0.77540165]] \n",
            "Iter 176/500 - Loss: 4.30120849609375 LenghtParam [[0.7758533]] \n",
            "Iter 177/500 - Loss: 4.296799659729004 LenghtParam [[0.77624315]] \n",
            "Iter 178/500 - Loss: 4.307877540588379 LenghtParam [[0.77665]] \n",
            "Iter 179/500 - Loss: 4.303927421569824 LenghtParam [[0.7770547]] \n",
            "Iter 180/500 - Loss: 4.303256511688232 LenghtParam [[0.777492]] \n",
            "Iter 181/500 - Loss: 4.306602954864502 LenghtParam [[0.77794677]] \n",
            "Iter 182/500 - Loss: 4.29865026473999 LenghtParam [[0.77843666]] \n",
            "Iter 183/500 - Loss: 4.302385330200195 LenghtParam [[0.7789092]] \n",
            "Iter 184/500 - Loss: 4.297069072723389 LenghtParam [[0.7793562]] \n",
            "Iter 185/500 - Loss: 4.304190635681152 LenghtParam [[0.77978057]] \n",
            "Iter 186/500 - Loss: 4.304328918457031 LenghtParam [[0.780203]] \n",
            "Iter 187/500 - Loss: 4.297736167907715 LenghtParam [[0.7806347]] \n",
            "Iter 188/500 - Loss: 4.300447463989258 LenghtParam [[0.7810699]] \n",
            "Iter 189/500 - Loss: 4.296931266784668 LenghtParam [[0.78150666]] \n",
            "Iter 190/500 - Loss: 4.30931282043457 LenghtParam [[0.7819376]] \n",
            "Iter 191/500 - Loss: 4.300988674163818 LenghtParam [[0.78233254]] \n",
            "Iter 192/500 - Loss: 4.298057556152344 LenghtParam [[0.7827651]] \n",
            "Iter 193/500 - Loss: 4.3007612228393555 LenghtParam [[0.78319085]] \n",
            "Iter 194/500 - Loss: 4.304069519042969 LenghtParam [[0.7836628]] \n",
            "Iter 195/500 - Loss: 4.300511837005615 LenghtParam [[0.78414005]] \n",
            "Iter 196/500 - Loss: 4.300778865814209 LenghtParam [[0.78463495]] \n",
            "Iter 197/500 - Loss: 4.298604965209961 LenghtParam [[0.7851251]] \n",
            "Iter 198/500 - Loss: 4.298814296722412 LenghtParam [[0.7855987]] \n",
            "Iter 199/500 - Loss: 4.2995219230651855 LenghtParam [[0.7860836]] \n",
            "Iter 200/500 - Loss: 4.299452304840088 LenghtParam [[0.7865485]] \n",
            "Iter 201/500 - Loss: 4.299760818481445 LenghtParam [[0.7869862]] \n",
            "Iter 202/500 - Loss: 4.306230545043945 LenghtParam [[0.78741366]] \n",
            "Iter 203/500 - Loss: 4.306407451629639 LenghtParam [[0.7878192]] \n",
            "Iter 204/500 - Loss: 4.302271842956543 LenghtParam [[0.7882693]] \n",
            "Iter 205/500 - Loss: 4.3106513023376465 LenghtParam [[0.78871745]] \n",
            "Iter 206/500 - Loss: 4.299874305725098 LenghtParam [[0.78918964]] \n",
            "Iter 207/500 - Loss: 4.297883987426758 LenghtParam [[0.7896694]] \n",
            "Iter 208/500 - Loss: 4.293079853057861 LenghtParam [[0.7901431]] \n",
            "Iter 209/500 - Loss: 4.301551818847656 LenghtParam [[0.7906299]] \n",
            "Iter 210/500 - Loss: 4.29733943939209 LenghtParam [[0.791099]] \n",
            "Iter 211/500 - Loss: 4.2938690185546875 LenghtParam [[0.79157436]] \n",
            "Iter 212/500 - Loss: 4.299014568328857 LenghtParam [[0.7920315]] \n",
            "Iter 213/500 - Loss: 4.299318790435791 LenghtParam [[0.79249185]] \n",
            "Iter 214/500 - Loss: 4.309662818908691 LenghtParam [[0.7929658]] \n",
            "Iter 215/500 - Loss: 4.301279067993164 LenghtParam [[0.7934452]] \n",
            "Iter 216/500 - Loss: 4.301749229431152 LenghtParam [[0.79391706]] \n",
            "Iter 217/500 - Loss: 4.303548336029053 LenghtParam [[0.79439473]] \n",
            "Iter 218/500 - Loss: 4.298120975494385 LenghtParam [[0.7948922]] \n",
            "Iter 219/500 - Loss: 4.295548915863037 LenghtParam [[0.7953815]] \n",
            "Iter 220/500 - Loss: 4.3016533851623535 LenghtParam [[0.7958323]] \n",
            "Iter 221/500 - Loss: 4.304813385009766 LenghtParam [[0.79629415]] \n",
            "Iter 222/500 - Loss: 4.30527400970459 LenghtParam [[0.79673517]] \n",
            "Iter 223/500 - Loss: 4.300764560699463 LenghtParam [[0.7971377]] \n",
            "Iter 224/500 - Loss: 4.311360836029053 LenghtParam [[0.7975536]] \n",
            "Iter 225/500 - Loss: 4.300572395324707 LenghtParam [[0.79800755]] \n",
            "Iter 226/500 - Loss: 4.300816535949707 LenghtParam [[0.7984514]] \n",
            "Iter 227/500 - Loss: 4.303932189941406 LenghtParam [[0.79888535]] \n",
            "Iter 228/500 - Loss: 4.3065876960754395 LenghtParam [[0.799326]] \n",
            "Iter 229/500 - Loss: 4.29949426651001 LenghtParam [[0.7997724]] \n",
            "Iter 230/500 - Loss: 4.305946350097656 LenghtParam [[0.80019796]] \n",
            "Iter 231/500 - Loss: 4.298443794250488 LenghtParam [[0.80069494]] \n",
            "Iter 232/500 - Loss: 4.309252738952637 LenghtParam [[0.80118877]] \n",
            "Iter 233/500 - Loss: 4.3022780418396 LenghtParam [[0.801672]] \n",
            "Iter 234/500 - Loss: 4.3040666580200195 LenghtParam [[0.8021732]] \n",
            "Iter 235/500 - Loss: 4.298312187194824 LenghtParam [[0.80266845]] \n",
            "Iter 236/500 - Loss: 4.309297561645508 LenghtParam [[0.803143]] \n",
            "Iter 237/500 - Loss: 4.302956581115723 LenghtParam [[0.8036348]] \n",
            "Iter 238/500 - Loss: 4.311899662017822 LenghtParam [[0.80415446]] \n",
            "Iter 239/500 - Loss: 4.297382831573486 LenghtParam [[0.8047057]] \n",
            "Iter 240/500 - Loss: 4.301013469696045 LenghtParam [[0.8052731]] \n",
            "Iter 241/500 - Loss: 4.301607608795166 LenghtParam [[0.80582166]] \n",
            "Iter 242/500 - Loss: 4.302372455596924 LenghtParam [[0.80637646]] \n",
            "Iter 243/500 - Loss: 4.294853687286377 LenghtParam [[0.8069128]] \n",
            "Iter 244/500 - Loss: 4.300378799438477 LenghtParam [[0.8074023]] \n",
            "Iter 245/500 - Loss: 4.299860954284668 LenghtParam [[0.80786306]] \n",
            "Iter 246/500 - Loss: 4.292476177215576 LenghtParam [[0.8083365]] \n",
            "Iter 247/500 - Loss: 4.296554088592529 LenghtParam [[0.8087787]] \n",
            "Iter 248/500 - Loss: 4.299527168273926 LenghtParam [[0.80922663]] \n",
            "Iter 249/500 - Loss: 4.303646564483643 LenghtParam [[0.8096938]] \n",
            "Iter 250/500 - Loss: 4.307981014251709 LenghtParam [[0.81020504]] \n",
            "Iter 251/500 - Loss: 4.3029608726501465 LenghtParam [[0.8107253]] \n",
            "Iter 252/500 - Loss: 4.295690059661865 LenghtParam [[0.8111929]] \n",
            "Iter 253/500 - Loss: 4.3029680252075195 LenghtParam [[0.8116375]] \n",
            "Iter 254/500 - Loss: 4.299410343170166 LenghtParam [[0.8120978]] \n",
            "Iter 255/500 - Loss: 4.3060760498046875 LenghtParam [[0.8125228]] \n",
            "Iter 256/500 - Loss: 4.299907684326172 LenghtParam [[0.81297463]] \n",
            "Iter 257/500 - Loss: 4.296370983123779 LenghtParam [[0.8133957]] \n",
            "Iter 258/500 - Loss: 4.3025994300842285 LenghtParam [[0.8137951]] \n",
            "Iter 259/500 - Loss: 4.306854724884033 LenghtParam [[0.8142519]] \n",
            "Iter 260/500 - Loss: 4.3028388023376465 LenghtParam [[0.81470156]] \n",
            "Iter 261/500 - Loss: 4.301543712615967 LenghtParam [[0.8151371]] \n",
            "Iter 262/500 - Loss: 4.307657718658447 LenghtParam [[0.8155735]] \n",
            "Iter 263/500 - Loss: 4.302390098571777 LenghtParam [[0.81608427]] \n",
            "Iter 264/500 - Loss: 4.302175521850586 LenghtParam [[0.8166]] \n",
            "Iter 265/500 - Loss: 4.308597564697266 LenghtParam [[0.8171128]] \n",
            "Iter 266/500 - Loss: 4.305352210998535 LenghtParam [[0.81765646]] \n",
            "Iter 267/500 - Loss: 4.3100199699401855 LenghtParam [[0.81819886]] \n",
            "Iter 268/500 - Loss: 4.301285266876221 LenghtParam [[0.81873816]] \n",
            "Iter 269/500 - Loss: 4.303954601287842 LenghtParam [[0.8192401]] \n",
            "Iter 270/500 - Loss: 4.299781799316406 LenghtParam [[0.81976163]] \n",
            "Iter 271/500 - Loss: 4.302853107452393 LenghtParam [[0.8203028]] \n",
            "Iter 272/500 - Loss: 4.304481506347656 LenghtParam [[0.8208529]] \n",
            "Iter 273/500 - Loss: 4.319635391235352 LenghtParam [[0.82136536]] \n",
            "Iter 274/500 - Loss: 4.298118591308594 LenghtParam [[0.8219277]] \n",
            "Iter 275/500 - Loss: 4.293354034423828 LenghtParam [[0.8224962]] \n",
            "Iter 276/500 - Loss: 4.3051018714904785 LenghtParam [[0.8230208]] \n",
            "Iter 277/500 - Loss: 4.306288719177246 LenghtParam [[0.82354546]] \n",
            "Iter 278/500 - Loss: 4.308194160461426 LenghtParam [[0.824116]] \n",
            "Iter 279/500 - Loss: 4.304022789001465 LenghtParam [[0.8247108]] \n",
            "Iter 280/500 - Loss: 4.304119110107422 LenghtParam [[0.82528615]] \n",
            "Iter 281/500 - Loss: 4.305734634399414 LenghtParam [[0.825858]] \n",
            "Iter 282/500 - Loss: 4.301586627960205 LenghtParam [[0.826467]] \n",
            "Iter 283/500 - Loss: 4.311141014099121 LenghtParam [[0.8270519]] \n",
            "Iter 284/500 - Loss: 4.298384189605713 LenghtParam [[0.8277084]] \n",
            "Iter 285/500 - Loss: 4.298480987548828 LenghtParam [[0.82832485]] \n",
            "Iter 286/500 - Loss: 4.297441005706787 LenghtParam [[0.8289338]] \n",
            "Iter 287/500 - Loss: 4.299542427062988 LenghtParam [[0.82947665]] \n",
            "Iter 288/500 - Loss: 4.303555011749268 LenghtParam [[0.83003867]] \n",
            "Iter 289/500 - Loss: 4.297070503234863 LenghtParam [[0.83055633]] \n",
            "Iter 290/500 - Loss: 4.298993110656738 LenghtParam [[0.8310307]] \n",
            "Iter 291/500 - Loss: 4.300422191619873 LenghtParam [[0.831498]] \n",
            "Iter 292/500 - Loss: 4.303530693054199 LenghtParam [[0.8319533]] \n",
            "Iter 293/500 - Loss: 4.303485870361328 LenghtParam [[0.83240956]] \n",
            "Iter 294/500 - Loss: 4.293655872344971 LenghtParam [[0.8328759]] \n",
            "Iter 295/500 - Loss: 4.305481910705566 LenghtParam [[0.8333403]] \n",
            "Iter 296/500 - Loss: 4.302623748779297 LenghtParam [[0.83380663]] \n",
            "Iter 297/500 - Loss: 4.29695987701416 LenghtParam [[0.8342561]] \n",
            "Iter 298/500 - Loss: 4.299898147583008 LenghtParam [[0.83470345]] \n",
            "Iter 299/500 - Loss: 4.299754619598389 LenghtParam [[0.8351726]] \n",
            "Iter 300/500 - Loss: 4.301403045654297 LenghtParam [[0.83569306]] \n",
            "Iter 301/500 - Loss: 4.300532341003418 LenghtParam [[0.8361982]] \n",
            "Iter 302/500 - Loss: 4.300248622894287 LenghtParam [[0.83670104]] \n",
            "Iter 303/500 - Loss: 4.3074564933776855 LenghtParam [[0.8372173]] \n",
            "Iter 304/500 - Loss: 4.303840637207031 LenghtParam [[0.83772314]] \n",
            "Iter 305/500 - Loss: 4.293139934539795 LenghtParam [[0.8381974]] \n",
            "Iter 306/500 - Loss: 4.303492546081543 LenghtParam [[0.838627]] \n",
            "Iter 307/500 - Loss: 4.2986369132995605 LenghtParam [[0.8391166]] \n",
            "Iter 308/500 - Loss: 4.301210880279541 LenghtParam [[0.839596]] \n",
            "Iter 309/500 - Loss: 4.296754360198975 LenghtParam [[0.84005183]] \n",
            "Iter 310/500 - Loss: 4.309732437133789 LenghtParam [[0.8405378]] \n",
            "Iter 311/500 - Loss: 4.301880836486816 LenghtParam [[0.841011]] \n",
            "Iter 312/500 - Loss: 4.29676628112793 LenghtParam [[0.84148544]] \n",
            "Iter 313/500 - Loss: 4.2983479499816895 LenghtParam [[0.8419552]] \n",
            "Iter 314/500 - Loss: 4.296863079071045 LenghtParam [[0.84243286]] \n",
            "Iter 315/500 - Loss: 4.300363063812256 LenghtParam [[0.8429155]] \n",
            "Iter 316/500 - Loss: 4.3065643310546875 LenghtParam [[0.84339696]] \n",
            "Iter 317/500 - Loss: 4.297945022583008 LenghtParam [[0.84393376]] \n",
            "Iter 318/500 - Loss: 4.301605224609375 LenghtParam [[0.84442496]] \n",
            "Iter 319/500 - Loss: 4.3020172119140625 LenghtParam [[0.8449047]] \n",
            "Iter 320/500 - Loss: 4.30169153213501 LenghtParam [[0.84538966]] \n",
            "Iter 321/500 - Loss: 4.305513381958008 LenghtParam [[0.8458885]] \n",
            "Iter 322/500 - Loss: 4.302582740783691 LenghtParam [[0.8463878]] \n",
            "Iter 323/500 - Loss: 4.296247482299805 LenghtParam [[0.84687185]] \n",
            "Iter 324/500 - Loss: 4.302112579345703 LenghtParam [[0.8473346]] \n",
            "Iter 325/500 - Loss: 4.311020374298096 LenghtParam [[0.84777427]] \n",
            "Iter 326/500 - Loss: 4.306171894073486 LenghtParam [[0.84827816]] \n",
            "Iter 327/500 - Loss: 4.307229995727539 LenghtParam [[0.84877867]] \n",
            "Iter 328/500 - Loss: 4.2988600730896 LenghtParam [[0.8492207]] \n",
            "Iter 329/500 - Loss: 4.299489974975586 LenghtParam [[0.84966815]] \n",
            "Iter 330/500 - Loss: 4.298488616943359 LenghtParam [[0.85012037]] \n",
            "Iter 331/500 - Loss: 4.295437812805176 LenghtParam [[0.8505343]] \n",
            "Iter 332/500 - Loss: 4.307846546173096 LenghtParam [[0.8509317]] \n",
            "Iter 333/500 - Loss: 4.296180725097656 LenghtParam [[0.8513611]] \n",
            "Iter 334/500 - Loss: 4.30559778213501 LenghtParam [[0.85177755]] \n",
            "Iter 335/500 - Loss: 4.296922206878662 LenghtParam [[0.8522426]] \n",
            "Iter 336/500 - Loss: 4.3037824630737305 LenghtParam [[0.8527018]] \n",
            "Iter 337/500 - Loss: 4.297574043273926 LenghtParam [[0.8531445]] \n",
            "Iter 338/500 - Loss: 4.303061008453369 LenghtParam [[0.8535951]] \n",
            "Iter 339/500 - Loss: 4.299962520599365 LenghtParam [[0.85405624]] \n",
            "Iter 340/500 - Loss: 4.301415920257568 LenghtParam [[0.8545344]] \n",
            "Iter 341/500 - Loss: 4.302548885345459 LenghtParam [[0.8550097]] \n",
            "Iter 342/500 - Loss: 4.300795078277588 LenghtParam [[0.85551804]] \n",
            "Iter 343/500 - Loss: 4.303927421569824 LenghtParam [[0.8560339]] \n",
            "Iter 344/500 - Loss: 4.302864074707031 LenghtParam [[0.85656106]] \n",
            "Iter 345/500 - Loss: 4.29990291595459 LenghtParam [[0.85702604]] \n",
            "Iter 346/500 - Loss: 4.297528266906738 LenghtParam [[0.8574923]] \n",
            "Iter 347/500 - Loss: 4.307369709014893 LenghtParam [[0.85791975]] \n",
            "Iter 348/500 - Loss: 4.300777435302734 LenghtParam [[0.85838753]] \n",
            "Iter 349/500 - Loss: 4.300021648406982 LenghtParam [[0.8588058]] \n",
            "Iter 350/500 - Loss: 4.296395778656006 LenghtParam [[0.8591888]] \n",
            "Iter 351/500 - Loss: 4.300397872924805 LenghtParam [[0.8596158]] \n",
            "Iter 352/500 - Loss: 4.2963666915893555 LenghtParam [[0.86007243]] \n",
            "Iter 353/500 - Loss: 4.301545143127441 LenghtParam [[0.860473]] \n",
            "Iter 354/500 - Loss: 4.29867696762085 LenghtParam [[0.8608729]] \n",
            "Iter 355/500 - Loss: 4.309190273284912 LenghtParam [[0.8612273]] \n",
            "Iter 356/500 - Loss: 4.299093246459961 LenghtParam [[0.86157316]] \n",
            "Iter 357/500 - Loss: 4.2996039390563965 LenghtParam [[0.8619402]] \n",
            "Iter 358/500 - Loss: 4.303513050079346 LenghtParam [[0.86228275]] \n",
            "Iter 359/500 - Loss: 4.30252742767334 LenghtParam [[0.86264944]] \n",
            "Iter 360/500 - Loss: 4.305188179016113 LenghtParam [[0.86301994]] \n",
            "Iter 361/500 - Loss: 4.307110786437988 LenghtParam [[0.86341804]] \n",
            "Iter 362/500 - Loss: 4.307429313659668 LenghtParam [[0.86380225]] \n",
            "Iter 363/500 - Loss: 4.297276973724365 LenghtParam [[0.86419696]] \n",
            "Iter 364/500 - Loss: 4.297721862792969 LenghtParam [[0.8645679]] \n",
            "Iter 365/500 - Loss: 4.298171520233154 LenghtParam [[0.8649457]] \n",
            "Iter 366/500 - Loss: 4.309176921844482 LenghtParam [[0.86532336]] \n",
            "Iter 367/500 - Loss: 4.2948899269104 LenghtParam [[0.8657617]] \n",
            "Iter 368/500 - Loss: 4.302741527557373 LenghtParam [[0.86617917]] \n",
            "Iter 369/500 - Loss: 4.302047252655029 LenghtParam [[0.86662835]] \n",
            "Iter 370/500 - Loss: 4.302981853485107 LenghtParam [[0.867074]] \n",
            "Iter 371/500 - Loss: 4.297610282897949 LenghtParam [[0.86751306]] \n",
            "Iter 372/500 - Loss: 4.3064866065979 LenghtParam [[0.8679578]] \n",
            "Iter 373/500 - Loss: 4.3052778244018555 LenghtParam [[0.86841184]] \n",
            "Iter 374/500 - Loss: 4.302609920501709 LenghtParam [[0.868887]] \n",
            "Iter 375/500 - Loss: 4.297529697418213 LenghtParam [[0.86932653]] \n",
            "Iter 376/500 - Loss: 4.297119140625 LenghtParam [[0.86976236]] \n",
            "Iter 377/500 - Loss: 4.298398494720459 LenghtParam [[0.8701938]] \n",
            "Iter 378/500 - Loss: 4.3007025718688965 LenghtParam [[0.8706284]] \n",
            "Iter 379/500 - Loss: 4.301076889038086 LenghtParam [[0.87108815]] \n",
            "Iter 380/500 - Loss: 4.295914649963379 LenghtParam [[0.87152183]] \n",
            "Iter 381/500 - Loss: 4.3038811683654785 LenghtParam [[0.87189746]] \n",
            "Iter 382/500 - Loss: 4.301395893096924 LenghtParam [[0.87231207]] \n",
            "Iter 383/500 - Loss: 4.305533409118652 LenghtParam [[0.8727355]] \n",
            "Iter 384/500 - Loss: 4.301263809204102 LenghtParam [[0.87320065]] \n",
            "Iter 385/500 - Loss: 4.300429344177246 LenghtParam [[0.8736409]] \n",
            "Iter 386/500 - Loss: 4.301969528198242 LenghtParam [[0.87413955]] \n",
            "Iter 387/500 - Loss: 4.297560214996338 LenghtParam [[0.8745733]] \n",
            "Iter 388/500 - Loss: 4.306392192840576 LenghtParam [[0.87497365]] \n",
            "Iter 389/500 - Loss: 4.298367023468018 LenghtParam [[0.87542075]] \n",
            "Iter 390/500 - Loss: 4.294283390045166 LenghtParam [[0.87584966]] \n",
            "Iter 391/500 - Loss: 4.297460079193115 LenghtParam [[0.8762919]] \n",
            "Iter 392/500 - Loss: 4.299808502197266 LenghtParam [[0.8767184]] \n",
            "Iter 393/500 - Loss: 4.301419734954834 LenghtParam [[0.8771733]] \n",
            "Iter 394/500 - Loss: 4.297357559204102 LenghtParam [[0.8776543]] \n",
            "Iter 395/500 - Loss: 4.300309658050537 LenghtParam [[0.8780952]] \n",
            "Iter 396/500 - Loss: 4.307895660400391 LenghtParam [[0.8785113]] \n",
            "Iter 397/500 - Loss: 4.298558712005615 LenghtParam [[0.878941]] \n",
            "Iter 398/500 - Loss: 4.309300422668457 LenghtParam [[0.8794108]] \n",
            "Iter 399/500 - Loss: 4.29970121383667 LenghtParam [[0.8798528]] \n",
            "Iter 400/500 - Loss: 4.296499729156494 LenghtParam [[0.88032186]] \n",
            "Iter 401/500 - Loss: 4.30298376083374 LenghtParam [[0.880794]] \n",
            "Iter 402/500 - Loss: 4.311756610870361 LenghtParam [[0.8812735]] \n",
            "Iter 403/500 - Loss: 4.3041534423828125 LenghtParam [[0.8816985]] \n",
            "Iter 404/500 - Loss: 4.310600280761719 LenghtParam [[0.8820929]] \n",
            "Iter 405/500 - Loss: 4.305905342102051 LenghtParam [[0.88252425]] \n",
            "Iter 406/500 - Loss: 4.296415328979492 LenghtParam [[0.8829354]] \n",
            "Iter 407/500 - Loss: 4.294419288635254 LenghtParam [[0.88337463]] \n",
            "Iter 408/500 - Loss: 4.304575443267822 LenghtParam [[0.8837931]] \n",
            "Iter 409/500 - Loss: 4.303110599517822 LenghtParam [[0.8842478]] \n",
            "Iter 410/500 - Loss: 4.299811363220215 LenghtParam [[0.88467366]] \n",
            "Iter 411/500 - Loss: 4.306313991546631 LenghtParam [[0.88507646]] \n",
            "Iter 412/500 - Loss: 4.305238723754883 LenghtParam [[0.8854915]] \n",
            "Iter 413/500 - Loss: 4.303410530090332 LenghtParam [[0.8859283]] \n",
            "Iter 414/500 - Loss: 4.298285007476807 LenghtParam [[0.886441]] \n",
            "Iter 415/500 - Loss: 4.300301551818848 LenghtParam [[0.8869256]] \n",
            "Iter 416/500 - Loss: 4.305190563201904 LenghtParam [[0.88741124]] \n",
            "Iter 417/500 - Loss: 4.303189277648926 LenghtParam [[0.88790375]] \n",
            "Iter 418/500 - Loss: 4.297675132751465 LenghtParam [[0.88842136]] \n",
            "Iter 419/500 - Loss: 4.302568435668945 LenghtParam [[0.8889092]] \n",
            "Iter 420/500 - Loss: 4.303188800811768 LenghtParam [[0.8894057]] \n",
            "Iter 421/500 - Loss: 4.303974151611328 LenghtParam [[0.88991433]] \n",
            "Iter 422/500 - Loss: 4.30379056930542 LenghtParam [[0.8904225]] \n",
            "Iter 423/500 - Loss: 4.297288417816162 LenghtParam [[0.89096403]] \n",
            "Iter 424/500 - Loss: 4.296042442321777 LenghtParam [[0.8914892]] \n",
            "Iter 425/500 - Loss: 4.294683456420898 LenghtParam [[0.8919947]] \n",
            "Iter 426/500 - Loss: 4.304304599761963 LenghtParam [[0.8924569]] \n",
            "Iter 427/500 - Loss: 4.3033671379089355 LenghtParam [[0.89286506]] \n",
            "Iter 428/500 - Loss: 4.306276321411133 LenghtParam [[0.8932582]] \n",
            "Iter 429/500 - Loss: 4.301249027252197 LenghtParam [[0.89373934]] \n",
            "Iter 430/500 - Loss: 4.298221588134766 LenghtParam [[0.89418113]] \n",
            "Iter 431/500 - Loss: 4.306554317474365 LenghtParam [[0.8946022]] \n",
            "Iter 432/500 - Loss: 4.302430629730225 LenghtParam [[0.89503205]] \n",
            "Iter 433/500 - Loss: 4.300727367401123 LenghtParam [[0.8954536]] \n",
            "Iter 434/500 - Loss: 4.2988481521606445 LenghtParam [[0.8958438]] \n",
            "Iter 435/500 - Loss: 4.304693698883057 LenghtParam [[0.8962135]] \n",
            "Iter 436/500 - Loss: 4.3006978034973145 LenghtParam [[0.89664644]] \n",
            "Iter 437/500 - Loss: 4.298182964324951 LenghtParam [[0.8970741]] \n",
            "Iter 438/500 - Loss: 4.301553726196289 LenghtParam [[0.89750034]] \n",
            "Iter 439/500 - Loss: 4.302066326141357 LenghtParam [[0.8978981]] \n",
            "Iter 440/500 - Loss: 4.307888984680176 LenghtParam [[0.8982837]] \n",
            "Iter 441/500 - Loss: 4.301173686981201 LenghtParam [[0.89867866]] \n",
            "Iter 442/500 - Loss: 4.299137592315674 LenghtParam [[0.89902496]] \n",
            "Iter 443/500 - Loss: 4.290915489196777 LenghtParam [[0.899366]] \n",
            "Iter 444/500 - Loss: 4.294896125793457 LenghtParam [[0.8997037]] \n",
            "Iter 445/500 - Loss: 4.306040287017822 LenghtParam [[0.9000135]] \n",
            "Iter 446/500 - Loss: 4.301893711090088 LenghtParam [[0.90029126]] \n",
            "Iter 447/500 - Loss: 4.305233478546143 LenghtParam [[0.90054965]] \n",
            "Iter 448/500 - Loss: 4.297323226928711 LenghtParam [[0.9008285]] \n",
            "Iter 449/500 - Loss: 4.301595687866211 LenghtParam [[0.90110415]] \n",
            "Iter 450/500 - Loss: 4.301576614379883 LenghtParam [[0.90142936]] \n",
            "Iter 451/500 - Loss: 4.30424165725708 LenghtParam [[0.9017455]] \n",
            "Iter 452/500 - Loss: 4.304170608520508 LenghtParam [[0.90211284]] \n",
            "Iter 453/500 - Loss: 4.309232234954834 LenghtParam [[0.9025664]] \n",
            "Iter 454/500 - Loss: 4.298872947692871 LenghtParam [[0.9030838]] \n",
            "Iter 455/500 - Loss: 4.300159454345703 LenghtParam [[0.9036216]] \n",
            "Iter 456/500 - Loss: 4.298141956329346 LenghtParam [[0.90412825]] \n",
            "Iter 457/500 - Loss: 4.294731140136719 LenghtParam [[0.9046029]] \n",
            "Iter 458/500 - Loss: 4.3001484870910645 LenghtParam [[0.9050601]] \n",
            "Iter 459/500 - Loss: 4.304371356964111 LenghtParam [[0.9054745]] \n",
            "Iter 460/500 - Loss: 4.305899143218994 LenghtParam [[0.90591574]] \n",
            "Iter 461/500 - Loss: 4.295261859893799 LenghtParam [[0.9064004]] \n",
            "Iter 462/500 - Loss: 4.297211170196533 LenghtParam [[0.9068369]] \n",
            "Iter 463/500 - Loss: 4.303389072418213 LenghtParam [[0.90722704]] \n",
            "Iter 464/500 - Loss: 4.305521011352539 LenghtParam [[0.90757054]] \n",
            "Iter 465/500 - Loss: 4.298726558685303 LenghtParam [[0.9078764]] \n",
            "Iter 466/500 - Loss: 4.295901298522949 LenghtParam [[0.90816164]] \n",
            "Iter 467/500 - Loss: 4.302456855773926 LenghtParam [[0.9085082]] \n",
            "Iter 468/500 - Loss: 4.300470352172852 LenghtParam [[0.9088422]] \n",
            "Iter 469/500 - Loss: 4.295435905456543 LenghtParam [[0.9091959]] \n",
            "Iter 470/500 - Loss: 4.305200576782227 LenghtParam [[0.90956086]] \n",
            "Iter 471/500 - Loss: 4.304355621337891 LenghtParam [[0.9098789]] \n",
            "Iter 472/500 - Loss: 4.300481796264648 LenghtParam [[0.9102486]] \n",
            "Iter 473/500 - Loss: 4.310536861419678 LenghtParam [[0.9106717]] \n",
            "Iter 474/500 - Loss: 4.307990074157715 LenghtParam [[0.91116834]] \n",
            "Iter 475/500 - Loss: 4.2952656745910645 LenghtParam [[0.91165733]] \n",
            "Iter 476/500 - Loss: 4.305254936218262 LenghtParam [[0.91211075]] \n",
            "Iter 477/500 - Loss: 4.303288459777832 LenghtParam [[0.91265124]] \n",
            "Iter 478/500 - Loss: 4.300380229949951 LenghtParam [[0.913203]] \n",
            "Iter 479/500 - Loss: 4.29994010925293 LenghtParam [[0.9137671]] \n",
            "Iter 480/500 - Loss: 4.306568145751953 LenghtParam [[0.91432786]] \n",
            "Iter 481/500 - Loss: 4.2995100021362305 LenghtParam [[0.91482824]] \n",
            "Iter 482/500 - Loss: 4.301784038543701 LenghtParam [[0.91532433]] \n",
            "Iter 483/500 - Loss: 4.2986016273498535 LenghtParam [[0.9157406]] \n",
            "Iter 484/500 - Loss: 4.307765007019043 LenghtParam [[0.91616607]] \n",
            "Iter 485/500 - Loss: 4.305401802062988 LenghtParam [[0.9167278]] \n",
            "Iter 486/500 - Loss: 4.299307823181152 LenghtParam [[0.9172687]] \n",
            "Iter 487/500 - Loss: 4.30541467666626 LenghtParam [[0.917809]] \n",
            "Iter 488/500 - Loss: 4.293036937713623 LenghtParam [[0.91840017]] \n",
            "Iter 489/500 - Loss: 4.303860187530518 LenghtParam [[0.91899306]] \n",
            "Iter 490/500 - Loss: 4.292293071746826 LenghtParam [[0.9196068]] \n",
            "Iter 491/500 - Loss: 4.299177646636963 LenghtParam [[0.9201641]] \n",
            "Iter 492/500 - Loss: 4.303197860717773 LenghtParam [[0.92069644]] \n",
            "Iter 493/500 - Loss: 4.295466899871826 LenghtParam [[0.9212273]] \n",
            "Iter 494/500 - Loss: 4.3031907081604 LenghtParam [[0.92174727]] \n",
            "Iter 495/500 - Loss: 4.29756498336792 LenghtParam [[0.9222664]] \n",
            "Iter 496/500 - Loss: 4.299293518066406 LenghtParam [[0.9228146]] \n",
            "Iter 497/500 - Loss: 4.297818183898926 LenghtParam [[0.92335844]] \n",
            "Iter 498/500 - Loss: 4.302707672119141 LenghtParam [[0.92386526]] \n",
            "Iter 499/500 - Loss: 4.3037800788879395 LenghtParam [[0.92436874]] \n",
            "Iter 500/500 - Loss: 4.300625801086426 LenghtParam [[0.9248612]] \n"
          ]
        }
      ],
      "source": [
        "## FIT THE MODEL\n",
        "noise_level = 0.01\n",
        "noise = noise_level*torch.ones(X_hifi.shape[0])\n",
        "likelihood = gpytorch.likelihoods.FixedNoiseGaussianLikelihood(noise=noise, learn_additional_noise=False)\n",
        "\n",
        "model = ExactGPModel(X_hifi, Y_hifi, likelihood)\n",
        "\n",
        "# HYPERPARAMETER TUNING\n",
        "model.train()\n",
        "likelihood.train()\n",
        "\n",
        "# Use the adam optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters())  # Includes GaussianLikelihood parameters\n",
        "\n",
        "# \"Loss\" for GPs - the marginal log likelihood\n",
        "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
        "training_iter  = 500\n",
        "for i in range(training_iter):\n",
        "    # Zero gradients from previous iteration\n",
        "    optimizer.zero_grad()\n",
        "    # Output from model\n",
        "    output = model(X_hifi)\n",
        "    # Calc loss and backprop gradients\n",
        "    loss = -mll(output, Y_hifi.squeeze())\n",
        "    loss.backward()\n",
        "\n",
        "    print('Iter {}/{} - Loss: {} LenghtParam {} '.format(\n",
        "        i + 1, training_iter, loss.item(), model.covar_module.base_kernel.lengthscale.detach().numpy() ))\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "qKyc4L8ETliT",
        "outputId": "e57b9d2a-407e-43dc-f28f-30e839abbf3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No of optimization: 0\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Flattening the training labels failed. The most common cause of this error is that the shapes of the prior mean and the training labels are mismatched. The shape of the train targets is torch.Size([1077, 1]), while the reported shape of the mean is torch.Size([1077]).",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gpytorch\\models\\exact_prediction_strategies.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, train_inputs, train_prior_dist, train_labels, likelihood, root, inv_root)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             train_labels = train_labels.reshape(\n\u001b[0m\u001b[0;32m     46\u001b[0m                 \u001b[1;33m*\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: shape '[1077, 1077]' is invalid for input of size 1077",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2504\\1183863501.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m    \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgpytorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfast_pred_var\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m       \u001b[0mobserved_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_candidates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m       \u001b[0mnew_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mobserved_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gpytorch\\models\\exact_gp.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m                 \u001b[1;31m# Create the prediction strategy for\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 284\u001b[1;33m                 self.prediction_strategy = prediction_strategy(\n\u001b[0m\u001b[0;32m    285\u001b[0m                     \u001b[0mtrain_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m                     \u001b[0mtrain_prior_dist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gpytorch\\models\\exact_prediction_strategies.py\u001b[0m in \u001b[0;36mprediction_strategy\u001b[1;34m(train_inputs, train_prior_dist, train_labels, likelihood)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDefaultPredictionStrategy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_prior_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gpytorch\\kernels\\scale_kernel.py\u001b[0m in \u001b[0;36mprediction_strategy\u001b[1;34m(self, train_inputs, train_prior_dist, train_labels, likelihood)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mprediction_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_prior_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_kernel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprediction_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_prior_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mc:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gpytorch\\kernels\\kernel.py\u001b[0m in \u001b[0;36mprediction_strategy\u001b[1;34m(self, train_inputs, train_prior_dist, train_labels, likelihood)\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mprediction_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_prior_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m         return exact_prediction_strategies.DefaultPredictionStrategy(\n\u001b[0m\u001b[0;32m    358\u001b[0m             \u001b[0mtrain_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_prior_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m         )\n",
            "\u001b[1;32mc:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gpytorch\\models\\exact_prediction_strategies.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, train_inputs, train_prior_dist, train_labels, likelihood, root, inv_root)\u001b[0m\n\u001b[0;32m     47\u001b[0m             )\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             raise RuntimeError(\n\u001b[0m\u001b[0;32m     50\u001b[0m                 \u001b[1;34m\"Flattening the training labels failed. The most common cause of this error is \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[1;33m+\u001b[0m \u001b[1;34m\"that the shapes of the prior mean and the training labels are mismatched. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Flattening the training labels failed. The most common cause of this error is that the shapes of the prior mean and the training labels are mismatched. The shape of the train targets is torch.Size([1077, 1]), while the reported shape of the mean is torch.Size([1077])."
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "likelihood.eval()\n",
        "\n",
        "n_runs = 1\n",
        "\n",
        "for i in range(n_runs):\n",
        "\n",
        "   print(f\"No of optimization: {i}\")\n",
        "   new_candidates = get_next_points(X_lofi, Y_lofi, best_obs_value, bounds, 1)\n",
        "\n",
        "   with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
        "      observed_pred = model(new_candidates[0])\n",
        "      new_results = torch.FloatTensor([observed_pred.mean.numpy()])\n",
        "\n",
        "   print(f\"New candidates are: {new_candidates}\")\n",
        "   X_lofi = torch.cat([X_lofi, new_candidates ])\n",
        "   Y_lofi = torch.cat([Y_lofi, new_results])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([120, 1])\n",
            "tensor([[-0.4339],\n",
            "        [ 0.8073],\n",
            "        [-0.4339],\n",
            "        [-1.9000]])\n",
            "torch.Size([1, 1])\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Flattening the training labels failed. The most common cause of this error is that the shapes of the prior mean and the training labels are mismatched. The shape of the train targets is torch.Size([1077, 1]), while the reported shape of the mean is torch.Size([1077]).",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gpytorch\\models\\exact_prediction_strategies.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, train_inputs, train_prior_dist, train_labels, likelihood, root, inv_root)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             train_labels = train_labels.reshape(\n\u001b[0m\u001b[0;32m     46\u001b[0m                 \u001b[1;33m*\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: shape '[1077, 1077]' is invalid for input of size 1077",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2504\\3836536255.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_candidates\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_candidates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mc:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gpytorch\\models\\exact_gp.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m                 \u001b[1;31m# Create the prediction strategy for\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 284\u001b[1;33m                 self.prediction_strategy = prediction_strategy(\n\u001b[0m\u001b[0;32m    285\u001b[0m                     \u001b[0mtrain_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m                     \u001b[0mtrain_prior_dist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gpytorch\\models\\exact_prediction_strategies.py\u001b[0m in \u001b[0;36mprediction_strategy\u001b[1;34m(train_inputs, train_prior_dist, train_labels, likelihood)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDefaultPredictionStrategy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_prior_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gpytorch\\kernels\\scale_kernel.py\u001b[0m in \u001b[0;36mprediction_strategy\u001b[1;34m(self, train_inputs, train_prior_dist, train_labels, likelihood)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mprediction_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_prior_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_kernel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprediction_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_prior_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mc:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gpytorch\\kernels\\kernel.py\u001b[0m in \u001b[0;36mprediction_strategy\u001b[1;34m(self, train_inputs, train_prior_dist, train_labels, likelihood)\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mprediction_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_prior_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m         return exact_prediction_strategies.DefaultPredictionStrategy(\n\u001b[0m\u001b[0;32m    358\u001b[0m             \u001b[0mtrain_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_prior_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m         )\n",
            "\u001b[1;32mc:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gpytorch\\models\\exact_prediction_strategies.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, train_inputs, train_prior_dist, train_labels, likelihood, root, inv_root)\u001b[0m\n\u001b[0;32m     47\u001b[0m             )\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             raise RuntimeError(\n\u001b[0m\u001b[0;32m     50\u001b[0m                 \u001b[1;34m\"Flattening the training labels failed. The most common cause of this error is \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[1;33m+\u001b[0m \u001b[1;34m\"that the shapes of the prior mean and the training labels are mismatched. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Flattening the training labels failed. The most common cause of this error is that the shapes of the prior mean and the training labels are mismatched. The shape of the train targets is torch.Size([1077, 1]), while the reported shape of the mean is torch.Size([1077])."
          ]
        }
      ],
      "source": [
        "print(X_lofi.size())\n",
        "print(torch.cat([X_lofi[0:3], new_candidates ]))\n",
        "print(new_candidates.size())\n",
        "with torch.no_grad():\n",
        "    print(model(new_candidates[0]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5 (tags/v3.9.5:0a7dcbd, May  3 2021, 17:27:52) [MSC v.1928 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "1cce27c8212bf6c5aa96f33a3d1153887721b66c5c8cb9adeaa83cce09196b75"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
